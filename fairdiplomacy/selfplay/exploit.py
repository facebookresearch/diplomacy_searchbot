from typing import Dict, Generator, List, Tuple, Sequence
import argparse
import collections
import itertools
import json
import logging
import pathlib
import time

import torch
import torch.multiprocessing as mp
import torch.utils.tensorboard

import diplomacy
from diplomacy.utils.export import to_saved_game_format, from_saved_game_format

import fairdiplomacy.selfplay.metrics
import fairdiplomacy.selfplay.vtrace
from fairdiplomacy.models.consts import POWERS, N_SCS
from fairdiplomacy.models.dipnet.load_model import load_dipnet_model
from fairdiplomacy.models.dipnet.order_vocabulary import EOS_IDX
from fairdiplomacy.utils.exception_handling_process import ExceptionHandlingProcess
from fairdiplomacy.selfplay.ckpt_syncer import CkptSyncer
from fairdiplomacy.selfplay.rollout import (
    ExploitRollout,
    InferencePool,
    yield_rollouts,
    order_logits_to_action_logprobs,
)
from fairdiplomacy.utils.timing_ctx import TimingCtx

# Path to save checkpoints to sync with the workers,
CKPT_SYNCER_PREFIX = "ckpt_syncer/ckpt"
# Save regular checkpoints.
CKPT_DIR = pathlib.Path("ckpt")
CKPT_TPL = "epoch%06d.ckpt"


RolloutBatch = collections.namedtuple(
    "RolloutBatch", "power_ids, observations, rewards, actions, logprobs, done"
)


def queue_rollouts(out_queue: mp.Queue, **rollout_kwargs) -> None:
    """A rollout worker function that syncs with outher workers and then produces N rollouts."""
    for item in yield_rollouts(**rollout_kwargs):
        out_queue.put(item)


def compute_rewards(power_id: int, game_json: Dict) -> torch.Tensor:
    """Compute given the game at the final stage."""
    # Phases includes one extra phase where nobody acted.
    num_active_phases = len(game_json["phases"]) - 1
    rewards = torch.zeros([num_active_phases])
    last_phase_centers = game_json["phases"][-1]["state"]["centers"]
    num_centers = len(last_phase_centers[POWERS[power_id]])
    rewards[-1] = num_centers / N_SCS
    return rewards


def compute_game_scores(power_id: int, game_json: Dict) -> Dict[str, float]:
    last_phase_centers = game_json["phases"][-1]["state"]["centers"]
    center_counts = [len(last_phase_centers[p]) for p in POWERS]
    center_suqares = [x ** 2 for x in center_counts]
    complete_unroll = game_json["phases"][-1]["name"] == "COMPLETED"
    metrics = dict(
        center_ratio_abs=center_counts[power_id] / N_SCS,
        square_ratio_rel=center_suqares[power_id] / sum(center_suqares, 1e-5),
        complete_unroll=float(complete_unroll),
        is_clear_win=float(center_counts[power_id] > N_SCS / 2),
        is_leader=float(center_counts[power_id] == max(center_counts)),
    )
    return metrics


def get_default_rollout_scores():
    return dict(
        center_ratio_abs=0,
        square_ratio_rel=0,
        complete_unroll=0,
        is_clear_win=0,
        is_leader=0,
        num_rollouts=0,
    )


def _join_batches(batches: Sequence[RolloutBatch]) -> RolloutBatch:
    merged = {}
    for k in RolloutBatch._fields:
        values = []
        for b in batches:
            values.append(getattr(b, k))
        if k == "observations":
            values = [torch.cat(x, 0) for x in zip(*values)]
        else:
            values = torch.cat(values, 0)
        merged[k] = values
    return RolloutBatch(**merged)


def yield_batches(
    model_path, num_rollout_processes, block_size, rollout_kwargs, dump_games, inference_batch_size
) -> Generator[Tuple[RolloutBatch, Dict[str, float]], None, None]:
    """Starts rollout processes and inference servers to generate impala data.

    Yields rollout in chunks. Each chunk contains concatenated
    block_size rollouts.

    Yields tuples (RolloutBatch, metrics).

    Metrics is a dict of some end-of-rollout metrics summed over all rollouts.
    """
    inference_gpus = [0, 1]
    inference_pool = InferencePool(
        model_path=model_path,
        gpu_ids=inference_gpus,
        ckpt_sync_path=CKPT_SYNCER_PREFIX,
        max_batch_size=inference_batch_size,
    )
    game_json = to_saved_game_format(diplomacy.Game())

    queue = mp.Queue()
    procs = [
        ExceptionHandlingProcess(
            target=queue_rollouts,
            args=[queue],
            kwargs=dict(
                initial_power_index=(i % len(POWERS)),
                blueprint_hostports=inference_pool.blueprint_agent_hostports,
                exploit_hostports=inference_pool.exploit_agent_hostports,
                game_json=game_json,
                **rollout_kwargs,
            ),
        )
        for i in range(num_rollout_processes)
    ]
    for p in procs:
        p.start()

    for step_id in itertools.count():
        rollouts_as_batches = []
        batch_rollout_scores = get_default_rollout_scores()

        for batch_id in range(block_size):
            rollout: ExploitRollout = queue.get()
            N = len(rollout.actions)
            if dump_games and batch_id == 0:
                game_dump_folder = pathlib.Path(f"dumped_games")
                game_dump_folder.mkdir(exist_ok=True, parents=True)
                dump_path = (
                    game_dump_folder / f"game.{POWERS[rollout.power_id]}.{step_id:06d}.json"
                )
                with (dump_path).open("w") as stream:
                    json.dump(rollout.game_json, stream)

            scores = compute_game_scores(rollout.power_id, rollout.game_json)
            for key, value in scores.items():
                batch_rollout_scores[key] += value
            batch_rollout_scores["num_rollouts"] += 1

            rewards = torch.zeros([N], dtype=torch.float)
            rewards[-1] = scores["center_ratio_abs"]

            is_final = torch.zeros([N], dtype=torch.bool)
            is_final[-1] = True

            rollouts_as_batches.append(
                RolloutBatch(
                    power_ids=torch.full([N], rollout.power_id, dtype=torch.long),
                    rewards=rewards,
                    observations=(rollout.observations),
                    actions=(rollout.actions),
                    logprobs=(rollout.logprobs),
                    done=is_final,
                )
            )
        yield _join_batches(rollouts_as_batches), batch_rollout_scores


def rebatch(
    batch_stream: Generator[Tuple[RolloutBatch, Dict[str, float]], None, None], batch_size: int
) -> Generator[Tuple[RolloutBatch, Dict[str, float]], None, None]:
    """Given a stream of batches (e.g., from yield_batches) repack into batches of fixed size."""

    accumulated_batches = []
    aggregated_scores = get_default_rollout_scores()
    size = 0
    for batch, scores in batch_stream:
        size += len(batch.rewards)
        accumulated_batches.append(batch)
        for k, v in scores.items():
            aggregated_scores[k] += v
        # Use strict > to simplify code.
        if size > batch_size:
            joined_batch = _join_batches(accumulated_batches)
            while size > batch_size:
                extracted_batch = fairdiplomacy.selfplay.metrics.rec_map(
                    lambda x: x[:batch_size], joined_batch
                )
                joined_batch = fairdiplomacy.selfplay.metrics.rec_map(
                    lambda x: x[batch_size:], joined_batch
                )
                size -= batch_size
                yield extracted_batch, aggregated_scores
                aggregated_scores = get_default_rollout_scores()
            accumulated_batches = [joined_batch]


def compute_baseline_loss(advantages):
    return 0.5 * (advantages ** 2).mean()


def compute_entropy_loss(logits, mask):
    """Return the entropy loss, i.e., the negative entropy of the policy."""
    policy = torch.nn.functional.softmax(logits, dim=-1)
    log_policy = torch.nn.functional.log_softmax(logits, dim=-1)
    return torch.sum(policy * log_policy * mask.unsqueeze(-1)) / (mask.sum() + 1e-5)


def compute_policy_gradient_loss(action_logprobs, advantages):
    return -torch.mean(action_logprobs * advantages.detach())


def to_onehot(indices, max_value):
    y_onehot = torch.zeros((len(indices), max_value), device=indices.device)
    y_onehot.scatter_(1, indices.unsqueeze(1), 1)
    return y_onehot


def build_optimizer(net, optimizer_cfg):
    return torch.optim.Adam(net.parameters(), lr=optimizer_cfg.lr)


def batched_index_select(input, dim, index):
    """Batched version of index_select with one dimensional index.

    output[b, ..pre.., ..post..] := input[b, ..pre.., index[b], ..post].
    """
    assert list(index.shape) == [input.shape[0]], (index.shape, input.shape)
    for _ in range(len(input.shape) - 1):
        index = index.unsqueeze(-1)
    expanse = list(input.shape)
    expanse[0] = -1
    expanse[dim] = -1
    index = index.expand(expanse)
    return torch.gather(input, dim, index).squeeze(dim)


def vtrace_from_logprobs_no_batch(**kwargs):
    kwargs = {k: (v.unsqueeze(1) if v.shape else v.unsqueeze(0)) for k, v in kwargs.items()}
    ret = fairdiplomacy.selfplay.vtrace.from_importance_weights(**kwargs)
    return type(ret)(*[i.squeeze(1) for i in ret])


class Logger:
    def __init__(self):
        # TODO(akhti): rank 0!
        self.writer = torch.utils.tensorboard.SummaryWriter(log_dir="tb")
        self.jsonl_writer = open("metrics.jsonl", "a")

    def log_metrics(self, metrics, step):
        for key, value in metrics.items():
            self.writer.add_scalar(key, value, global_step=step)
        print(json.dumps(dict(epoch=step, **metrics)), file=self.jsonl_writer, flush=True)

    def __del__(self):
        if self.writer is not None:
            self.writer.close()
            self.writer = None
        if self.jsonl_writer is not None:
            self.jsonl_writer.close()
            self.jsonl_writer = None


class ExploitTrainer(object):
    def __init__(self, cfg):
        self.cfg = cfg
        net = load_dipnet_model(cfg.model_path, map_location="cuda", eval=True)
        optim = build_optimizer(net, cfg.optimizer)
        self.state = argparse.Namespace(
            model=net,
            optimizer=optim,
            epoch_id=0,
            global_step=0,
            args=torch.load(cfg.model_path)["args"],
        )

        # TODO(akhti): handle multi machine.
        CKPT_DIR.mkdir(exist_ok=True, parents=True)
        if list(CKPT_DIR.iterdir()):
            *_, last_ckpt = sorted(CKPT_DIR.iterdir())
            logging.info("Found existing checkpoint folder. Will load last one: %s", last_ckpt)
            self.load_state(last_ckpt)

    def save_state(self, path):
        serialized_state = {}
        for k, v in vars(self.state).items():
            if hasattr(v, "state_dict"):
                serialized_state[k] = v.state_dict()
            else:
                serialized_state[k] = v
        torch.save(serialized_state, path)

    def load_state(self, path):
        serialized_state = torch.load(path)
        if frozenset(vars(self.state)) == frozenset(serialized_state):
            logging.error(
                "Loading state from '%s' that has different set of keys.\n\tState keys: %s\n\tckpt keys:%s",
                path,
                sorted(vars(self.state)),
                sorted(serialized_state),
            )
            raise ValueError("Bad checkpoint")
        for k, v in serialized_state.items():
            if hasattr(getattr(self.state, k), "load_state_dict"):
                getattr(self.state, k).load_state_dict(v)
            else:
                setattr(self.state, k, v)

    def run(self):
        cfg = self.cfg

        # TODO(akhti): wipe the folder.
        # TODO(akhti): handle multi machine.
        ckpt_syncer = CkptSyncer(CKPT_SYNCER_PREFIX, create_dir=True)

        logger = Logger()

        # Number of the rollout processes.
        num_rollout_processes = cfg.rollout.num_rollout_processes
        rollout_kwargs = dict(
            max_rollout_length=cfg.rollout.max_rollout_length, batch_size=cfg.rollout.batch_size
        )
        # Each procces makes a single rollout for a single power.
        ckpt_syncer.save_state_dict(self.state.model)
        data_loader = yield_batches(
            cfg.model_path,
            num_rollout_processes,
            cfg.rollout.block_size,
            rollout_kwargs,
            cfg.rollout.dump_games,
            cfg.rollout.inference_batch_size,
        )
        if self.cfg.rollout.rebatch_size:
            data_loader = rebatch(data_loader, self.cfg.rollout.rebatch_size)
        data_loader = iter(data_loader)
        self.state.model.train()
        max_epochs = self.cfg.trainer.max_epochs or 10 ** 9
        for self.state.epoch_id in range(self.state.epoch_id, max_epochs):
            if (
                self.cfg.trainer.save_checkpoint_every
                and self.state.epoch_id % self.cfg.trainer.save_checkpoint_every == 0
            ):
                self.save_state(CKPT_DIR / (CKPT_TPL % self.state.epoch_id))
            # Coutner accumulate different statistic over the epoch. Default
            # accumulation strategy is averaging.
            counters = collections.defaultdict(fairdiplomacy.selfplay.metrics.FractionCounter)
            counters[
                "optim/grad_max"
            ] = grad_max_counter = fairdiplomacy.selfplay.metrics.MaxCounter()
            # For LR just record it's value at the start of the epoch.
            counters["optim/lr"].update(next(iter(self.state.optimizer.param_groups))["lr"])
            epoch_start_time = time.time()
            for _ in range(self.cfg.trainer.epoch_size):
                timings = TimingCtx()
                with timings("data_gen"):
                    (
                        (power_ids, obs, rewards, actions, behavior_action_logprobs, done),
                        rollout_scores,
                    ) = next(data_loader)

                    if self.cfg.delay_penalty:
                        # TODO(akhti): move this up.
                        rewards -= self.cfg.delay_penalty

                with timings("to_cuda"):
                    actions = actions.cuda()
                    rewards = rewards.cuda()
                    power_ids = power_ids.cuda()
                    obs = [x.cuda() for x in obs]
                    behavior_action_logprobs = behavior_action_logprobs.cuda()
                    done = done.cuda()

                with timings("net"):
                    # TODO(akhti): move up state?
                    # As we are using x_power, we have to remove input information for
                    # other powers.
                    obs[4] = batched_index_select(obs[4], 1, power_ids)
                    obs[5] = batched_index_select(obs[5], 1, power_ids)

                    # Shape: _, [B, 17, 13k], [B, 7].
                    _, policy_logits, sc_values = self.state.model(
                        *obs,
                        temperature=1.0,
                        teacher_force_orders=actions,
                        x_power=to_onehot(power_ids, len(POWERS)),
                    )
                    # Shape: [B].
                    sc_values = sc_values.gather(1, power_ids.unsqueeze(1)).squeeze(1) / N_SCS

                    if self.cfg.rollout.rebatch_size:
                        # Reducing batch size by one. Deleting things that are
                        # too lazy to adjsut to avoid artifacts.
                        bootstrap_value = sc_values[-1].detach()
                        sc_values = sc_values[:-1]
                        policy_logits = policy_logits[:-1]
                        actions = actions[:-1]
                        rewards = rewards[:-1]
                        del obs
                        behavior_action_logprobs = behavior_action_logprobs[:-1]
                        done = done[:-1]
                    else:
                        # Asssumes that episode actually ends.
                        bootstrap_value = torch.zeros_like(sc_values[-1])

                    # Shape: [B].
                    discounts = (~done).float() * self.cfg.discounting

                    # Shape: [B, 17].
                    mask = (actions != EOS_IDX).float()

                    policy_action_logprobs = order_logits_to_action_logprobs(
                        policy_logits, actions, mask
                    )

                    vtrace_returns = vtrace_from_logprobs_no_batch(
                        log_rhos=policy_action_logprobs - behavior_action_logprobs,
                        discounts=discounts,
                        rewards=rewards,
                        values=sc_values,
                        bootstrap_value=bootstrap_value,
                    )

                    losses = dict(
                        actor=compute_policy_gradient_loss(
                            policy_action_logprobs, vtrace_returns.pg_advantages
                        ),
                        critic=compute_baseline_loss(vtrace_returns.vs - sc_values),
                        # TODO(akhti): it's incorrect to apply this to
                        # per-position order distribution instead of action
                        # distribution.
                        entropy=compute_entropy_loss(policy_logits, mask),
                    )

                    loss = (
                        losses["actor"]
                        + cfg.critic_weight * losses["critic"]
                        + cfg.entropy_weight * losses["entropy"]
                    )

                    self.state.optimizer.zero_grad()
                    loss.backward()

                    if self.cfg.optimizer.grad_clip > 1e-10:
                        g_norm = torch.nn.utils.clip_grad_norm_(
                            self.state.model.parameters(), self.cfg.optimizer.grad_clip
                        )
                        grad_max_counter.update(g_norm)
                        counters["optim/grad_mean"].update(g_norm)
                        counters["optim/grad_clip_ratio"].update(
                            int(g_norm >= self.cfg.optimizer.grad_clip - 1e-5)
                        )

                    self.state.optimizer.step()
                    # Sync to make sure timing is correct.
                    loss.item()

                with timings("metrics_and_sync"), torch.no_grad():
                    for key, value in losses.items():
                        counters[f"loss/{key}"].update(value)
                    counters[f"loss/total"].update(loss.item())
                    for key, value in rollout_scores.items():
                        if key != "num_rollouts":
                            counters[f"score/{key}"].update(value, rollout_scores["num_rollouts"])

                    counters["reward/mean"].update(rewards.sum(), len(rewards))
                    # Rewards at the end of episodes.
                    last_sum = rewards[done].sum()
                    last_count = rewards[done].numel()
                    counters["reward/last"].update(last_sum, last_count)
                    # To match entropy loss we don't negate logprobs. So this
                    # is an estimate of the negative entropy.
                    counters["loss/entropy_sampled"].update(policy_action_logprobs.mean())

                    bsz = len(actions)
                    counters["size/batch"].update(bsz)
                    counters["size/episode"].update(bsz, last_count)

                    ckpt_syncer.save_state_dict(self.state.model)

                # Doing outside of the context to capture the context's timing.
                for key, value in timings.items():
                    counters[f"time/{key}"].update(value)

                if (
                    self.state.global_step < 128
                    or (self.state.global_step & self.state.global_step + 1) == 0
                ):
                    logging.info(
                        "Metrics (global_step=%d): %s",
                        self.state.global_step,
                        {k: v.value() for k, v in sorted(counters.items())},
                    )
                self.state.global_step += 1

            epoch_scalars = {k: v.value() for k, v in sorted(counters.items())}
            epoch_scalars["speed/loop_bps"] = self.cfg.trainer.epoch_size / (
                time.time() - epoch_start_time + 1e-5
            )
            epoch_scalars["speed/loop_eps"] = (
                epoch_scalars["speed/loop_bps"] * epoch_scalars["batch_size"]
            )
            # Speed for to_cuda + forward + backward.
            epoch_scalars["speed/train_bps"] = 1.0 / (
                epoch_scalars["times/net"] + epoch_scalars["times/to_cuda"]
            )
            epoch_scalars["speed/train_eps"] = epoch_scalars["batch_size"] / (
                epoch_scalars["times/net"] + epoch_scalars["times/to_cuda"]
            )
            logging.info("Finished epoch %d. Metrics: %s", self.state.epoch_id, epoch_scalars)
            logger.log_metrics(epoch_scalars, self.state.epoch_id)


def resolve_and_flatten(nested_dict_of_scalar_tensors):
    return fairdiplomacy.selfplay.metrics.flatten_dict(
        fairdiplomacy.selfplay.metrics.recursive_tensor_item(nested_dict_of_scalar_tensors)
    )


def task(cfg):
    ExploitTrainer(cfg).run()
