from typing import Dict
import argparse
import collections
import logging
import pathlib
import time

import attr
import torch
import torch.utils.tensorboard


import fairdiplomacy.selfplay.data_loader
import fairdiplomacy.selfplay.metrics
import fairdiplomacy.selfplay.vtrace
from fairdiplomacy.models.consts import POWERS
from fairdiplomacy.models.dipnet.load_model import load_dipnet_model
from fairdiplomacy.models.dipnet.order_vocabulary import EOS_IDX
from fairdiplomacy.selfplay.ckpt_syncer import CkptSyncer
from fairdiplomacy.utils.timing_ctx import TimingCtx
from fairdiplomacy.selfplay.rollout import order_logits_to_action_logprobs
import heyhi

# Save regular checkpoints.
CKPT_DIR = pathlib.Path("ckpt")
CKPT_TPL = "epoch%06d.ckpt"

REQUEUE_CKPT = pathlib.Path("requeue.ckpt")


ScoreDict = Dict[str, float]


def compute_entropy_loss(logits, mask):
    """Return the entropy loss, i.e., the negative entropy of the policy."""
    policy = torch.nn.functional.softmax(logits, dim=-1)
    log_policy = torch.nn.functional.log_softmax(logits, dim=-1)
    return torch.sum(policy * log_policy * mask.unsqueeze(-1)) / (mask.sum() + 1e-5)


def compute_policy_gradient_loss(action_logprobs, advantages):
    return -torch.mean(action_logprobs * advantages.detach())


def to_onehot(indices, max_value):
    y_onehot = torch.zeros((len(indices), max_value), device=indices.device)
    y_onehot.scatter_(1, indices.unsqueeze(1), 1)
    return y_onehot


def build_optimizer(net, optimizer_cfg):
    return torch.optim.Adam(net.parameters(), lr=optimizer_cfg.lr)


def vtrace_from_logprobs_no_batch(**kwargs):
    kwargs = {k: (v.unsqueeze(1) if v.shape else v.unsqueeze(0)) for k, v in kwargs.items()}
    ret = fairdiplomacy.selfplay.vtrace.from_importance_weights(**kwargs)
    return type(ret)(*[i.squeeze(1) for i in ret])


def clip_grad_norm_(parameters, max_norm, norm_type=2):
    """Copied from Pytorch 1.5. Faster version for grad norm."""
    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]
    parameters = list(filter(lambda p: p.grad is not None, parameters))
    max_norm = float(max_norm)
    norm_type = float(norm_type)
    total_norm = torch.norm(
        torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]), norm_type
    )
    clip_coef = max_norm / (total_norm + 1e-6)
    if clip_coef < 1:
        for p in parameters:
            p.grad.detach().mul_(clip_coef)
    return total_norm


@attr.s(auto_attribs=True)
class TrainerState:
    """
    Contains the state of the Trainer.
    It can be saved to checkpoint the training and loaded to resume it.
    """

    epoch_id: int
    global_step: int
    model: torch.nn.Module
    optimizer: torch.optim.Optimizer
    # To be able to load the model in eval.
    args: argparse.Namespace

    def state_dict(self) -> Dict:
        data = attr.asdict(self)
        data["model"] = self.model.state_dict()
        data["optimizer"] = self.optimizer.state_dict()
        return data

    def save(self, filename: pathlib.Path) -> None:
        tmp_fpath = pathlib.Path(str(filename) + ".tmp")
        torch.save(self.state_dict(), tmp_fpath)
        tmp_fpath.rename(filename)

    @classmethod
    def load(
        cls, filename: pathlib.Path, default: "TrainerState", device: str = "cpu"
    ) -> "TrainerState":
        data = torch.load(filename, map_location=device)
        if frozenset(attr.asdict(default)) != frozenset(data):
            logging.error(
                "Loading state from '%s' that has different set of keys.\n\tState keys: %s\n\tckpt keys:%s",
                filename,
                sorted(attr.asdict(default)),
                sorted(data),
            )
            raise ValueError("Bad checkpoint")
        # We need this default to load the state dict
        model = default.model
        model.load_state_dict(data["model"])
        data["model"] = model

        optimizer = default.optimizer
        optimizer.load_state_dict(data["optimizer"])
        data["optimizer"] = optimizer

        logging.info("Loaded state from %s", filename)
        logging.info(
            "Loaded scalars: %s",
            {k: v for k, v in data.items() if isinstance(v, (int, float, str))},
        )

        return cls(**data)


class ExploitTrainer:
    def __init__(self, cfg: "conf.conf_pb2.ExploitTask"):
        self.cfg = cfg
        if cfg.seed > 0:
            torch.manual_seed(cfg.seed)
        self.state: TrainerState
        self.logger: fairdiplomacy.selfplay.metrics.Logger
        self.last_epoch_state: Dict

    def _init_state(self, device):
        net = load_dipnet_model(self.cfg.model_path, map_location=device, eval=True)
        if self.cfg.reset_agent_weights:

            def _reset(module):
                if hasattr(module, "reset_parameters"):
                    module.reset_parameters()

            net.apply(_reset)

        optim = build_optimizer(net, self.cfg.optimizer)
        self.state = TrainerState(
            model=net,
            optimizer=optim,
            epoch_id=0,
            global_step=0,
            args=torch.load(self.cfg.model_path)["args"],
        )

    def on_requeue(self):
        logging.info("Pre-termination callback")
        if hasattr(self, "logger"):
            logging.info("Closing the logger")
            self.logger.close()
        if hasattr(self, "data_loader"):
            logging.info("Killing data loader")
            self.data_loader.terminate()

    def __call__(self):
        cfg = self.cfg
        device = "cuda"  # Training device.
        self._init_state(device)

        if heyhi.is_master():
            CKPT_DIR.mkdir(exist_ok=True, parents=True)
        if REQUEUE_CKPT.exists():
            logging.info("Found requeue checkpoint: %s", REQUEUE_CKPT.resolve())
            self.state = TrainerState.load(REQUEUE_CKPT, self.state, device)
        elif CKPT_DIR.exists() and list(CKPT_DIR.iterdir()):
            last_ckpt = max(CKPT_DIR.iterdir(), key=str)
            logging.info("Found existing checkpoint folder. Will load last one: %s", last_ckpt)
            self.state = TrainerState.load(last_ckpt, self.state, device)
        else:
            logging.info("No checkpoints to restart from found")

        # TODO(akhti): wipe the folder.
        # TODO(akhti): handle multi machine.
        ckpt_syncer = CkptSyncer(
            fairdiplomacy.selfplay.data_loader.get_ckpt_sync_dir(), create_dir=True
        )
        ckpt_syncer.save_state_dict(self.state.model)

        logger = self.logger = fairdiplomacy.selfplay.metrics.Logger()
        data_loader = self.data_loader = fairdiplomacy.selfplay.data_loader.DataLoader(
            cfg.model_path, cfg.rollout
        )
        self.state.model.train()
        if self.cfg.trainer.train_as_eval:
            self.state.model.eval()
            # Cast cuDNN RNN back to train mode.
            self.state.model.apply(_lstm_to_train)
        elif self.cfg.trainer.train_encoder_as_eval:
            self.state.model.encoder.eval()
        elif self.cfg.trainer.train_decoder_as_eval:
            self.state.model.policy_decoder.eval()
            self.state.model.policy_decoder.apply(_lstm_to_train)
        elif self.cfg.trainer.train_as_eval_but_batchnorm:
            self.state.model.eval()
            self.state.model.apply(_lstm_to_train)
            self.state.model.apply(_bn_to_train)

        max_epochs = self.cfg.trainer.max_epochs or 10 ** 9
        self.state.model.to(device)

        # Installing requeue handle at the last moment so that we don't requeue zombie job.
        heyhi.maybe_init_requeue_handler(self.on_requeue)

        for self.state.epoch_id in range(self.state.epoch_id, max_epochs):
            # Clone state each epoch in case we'll need to requeue.
            self.state.save(REQUEUE_CKPT)
            if (
                self.cfg.trainer.save_checkpoint_every
                and self.state.epoch_id % self.cfg.trainer.save_checkpoint_every == 0
            ):
                self.state.save(CKPT_DIR / (CKPT_TPL % self.state.epoch_id))
            # Coutner accumulate different statistic over the epoch. Default
            # accumulation strategy is averaging.
            counters = collections.defaultdict(fairdiplomacy.selfplay.metrics.FractionCounter)
            use_grad_clip = self.cfg.optimizer.grad_clip > 1e-10
            if use_grad_clip:
                counters[
                    "optim/grad_max"
                ] = grad_max_counter = fairdiplomacy.selfplay.metrics.MaxCounter()
            # For LR just record it's value at the start of the epoch.
            counters["optim/lr"].update(next(iter(self.state.optimizer.param_groups))["lr"])
            epoch_start_time = time.time()
            for _ in range(self.cfg.trainer.epoch_size):
                timings = TimingCtx()
                with timings("data_gen"):
                    (
                        (power_ids, obs, rewards, actions, behavior_action_logprobs, done),
                        rollout_scores,
                    ) = data_loader.get_batch()

                with timings("to_cuda"):
                    actions = actions.to(device)
                    rewards = rewards.to(device)
                    power_ids = power_ids.to(device)
                    *obs, cand_actions = [x.to(device) for x in obs]
                    behavior_action_logprobs = behavior_action_logprobs.to(device)
                    done = done.to(device)

                with timings("net"):
                    # Shape: _, [B, 17], [B, S, 469], [B, 7].
                    # policy_cand_actions has the same information as actions,
                    # but uses local indices to match policy logits.
                    assert EOS_IDX == -1, "Rewrite the code to remove the assumption"
                    _, _, policy_logits, sc_values = self.state.model(
                        *obs,
                        temperature=1.0,
                        teacher_force_orders=actions.clamp(0),  # EOS_IDX = -1 -> 0
                        x_power=to_onehot(power_ids, len(POWERS)),
                    )
                    cand_actions = cand_actions[:, : policy_logits.shape[1]]

                    # Shape: [B].
                    sc_values = sc_values.gather(1, power_ids.unsqueeze(1)).squeeze(1)

                    # Removing absolute order ids to not use them by accident.
                    # Will use relative order ids (cand_actions) from now on.
                    del actions

                    if self.cfg.rollout.do_not_split_rollouts:
                        # Asssumes that episode actually ends.
                        bootstrap_value = torch.zeros_like(sc_values[-1])
                    else:
                        # Reducing batch size by one. Deleting things that are
                        # too lazy to adjsut to avoid artifacts.
                        bootstrap_value = sc_values[-1].detach()
                        sc_values = sc_values[:-1]
                        cand_actions = cand_actions[:-1]
                        policy_logits = policy_logits[:-1]
                        rewards = rewards[:-1]
                        del obs
                        del power_ids
                        behavior_action_logprobs = behavior_action_logprobs[:-1]
                        done = done[:-1]

                    # Shape: [B].
                    discounts = (~done).float() * self.cfg.discounting

                    # Shape: [B, 17].
                    mask = (cand_actions != EOS_IDX).float()

                    policy_action_logprobs = order_logits_to_action_logprobs(
                        policy_logits, cand_actions, mask
                    )

                    vtrace_returns = vtrace_from_logprobs_no_batch(
                        log_rhos=policy_action_logprobs - behavior_action_logprobs,
                        discounts=discounts,
                        rewards=rewards,
                        values=sc_values,
                        bootstrap_value=bootstrap_value,
                    )

                    critic_mses = 0.5 * ((vtrace_returns.vs.detach() - sc_values) ** 2)

                    losses = dict(
                        actor=compute_policy_gradient_loss(
                            policy_action_logprobs, vtrace_returns.pg_advantages
                        ),
                        critic=critic_mses.mean(),
                        # TODO(akhti): it's incorrect to apply this to
                        # per-position order distribution instead of action
                        # distribution.
                        entropy=compute_entropy_loss(policy_logits, mask),
                    )

                    loss = (
                        losses["actor"]
                        + cfg.critic_weight * losses["critic"]
                        + cfg.entropy_weight * losses["entropy"]
                    )

                    self.state.optimizer.zero_grad()
                    loss.backward()

                    if use_grad_clip:
                        g_norm_tensor = clip_grad_norm_(
                            self.state.model.parameters(), self.cfg.optimizer.grad_clip
                        )

                    if (
                        not self.cfg.trainer.max_updates
                        or self.state.global_step < self.cfg.trainer.max_updates
                    ):
                        self.state.optimizer.step()
                    # Sync to make sure timing is correct.
                    loss.item()

                with timings("metrics"), torch.no_grad():
                    last_count = done.long().sum()
                    critic_end_mses = critic_mses[done].sum()

                    if use_grad_clip:
                        g_norm = g_norm_tensor.item()
                        grad_max_counter.update(g_norm)
                        counters["optim/grad_mean"].update(g_norm)
                        counters["optim/grad_clip_ratio"].update(
                            int(g_norm >= self.cfg.optimizer.grad_clip - 1e-5)
                        )
                    for key, value in losses.items():
                        counters[f"loss/{key}"].update(value)
                    counters[f"loss/total"].update(loss.item())
                    for key, value in rollout_scores.items():
                        if key != "num_games":
                            counters[f"score/{key}"].update(value, rollout_scores["num_games"])

                    counters["loss/critic_last"].update(critic_end_mses, last_count)

                    counters["reward/mean"].update(rewards.sum(), len(rewards))
                    # Rewards at the end of episodes.
                    last_sum = rewards[done].sum()
                    counters["reward/last"].update(last_sum, last_count)
                    # To match entropy loss we don't negate logprobs. So this
                    # is an estimate of the negative entropy.
                    counters["loss/entropy_sampled"].update(policy_action_logprobs.mean())

                    # Measure off-policiness.
                    counters["loss/rho"].update(
                        vtrace_returns.rhos.sum(), vtrace_returns.rhos.numel()
                    )
                    counters["loss/rhos_clipped"].update(
                        vtrace_returns.clipped_rhos.sum(), vtrace_returns.clipped_rhos.numel()
                    )

                    bsz = len(rewards)
                    counters["size/batch"].update(bsz)
                    counters["size/episode"].update(bsz, last_count)

                with timings("sync"), torch.no_grad():
                    if self.state.global_step % self.cfg.trainer.save_sync_checkpoint_every == 0:
                        ckpt_syncer.save_state_dict(self.state.model)

                # Doing outside of the context to capture the context's timing.
                for key, value in timings.items():
                    counters[f"time/{key}"].update(value)

                if (
                    self.state.global_step < 128
                    or (self.state.global_step & self.state.global_step + 1) == 0
                ):
                    logging.info(
                        "Metrics (global_step=%d): %s",
                        self.state.global_step,
                        {k: v.value() for k, v in sorted(counters.items())},
                    )
                self.state.global_step += 1

            epoch_scalars = {k: v.value() for k, v in sorted(counters.items())}
            average_batch_size = epoch_scalars["size/batch"]
            epoch_scalars["speed/loop_bps"] = self.cfg.trainer.epoch_size / (
                time.time() - epoch_start_time + 1e-5
            )
            epoch_scalars["speed/loop_eps"] = epoch_scalars["speed/loop_bps"] * average_batch_size
            # Speed for to_cuda + forward + backward.
            torch_time = epoch_scalars["time/net"] + epoch_scalars["time/to_cuda"]
            epoch_scalars["speed/train_bps"] = 1.0 / torch_time
            epoch_scalars["speed/train_eps"] = average_batch_size / torch_time

            eval_scores = data_loader.extract_eval_scores()
            if eval_scores is not None:
                for k, v in eval_scores.items():
                    epoch_scalars[f"score_eval/{k}"] = v

            logging.info("Finished epoch %d. Metrics: %s", self.state.epoch_id, epoch_scalars)
            logger.log_metrics(epoch_scalars, self.state.epoch_id)
        logging.info("End of training")
        data_loader.terminate()
        logging.info("Exiting main funcion")


def task(cfg):
    ExploitTrainer(cfg)()


def _lstm_to_train(module):
    if module.__class__.__name__ == "LSTM":
        module.train()


def _bn_to_train(module):
    if "BatchNorm" in module.__class__.__name__:
        module.train()
