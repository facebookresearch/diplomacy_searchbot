from typing import Dict, Generator, List, Optional, Tuple, Sequence
import argparse
import collections
import datetime
import itertools
import json
import logging
import multiprocessing as mp
import pathlib
import time
import queue as queue_lib

from google.protobuf.json_format import MessageToDict
import torch
import torch.utils.tensorboard


import fairdiplomacy.selfplay.metrics
import fairdiplomacy.selfplay.vtrace
import fairdiplomacy.utils.game_scoring
from fairdiplomacy.game import Game
from fairdiplomacy.models.consts import POWERS
from fairdiplomacy.models.dipnet.load_model import load_dipnet_model
from fairdiplomacy.models.dipnet.order_vocabulary import EOS_IDX
from fairdiplomacy.utils.exception_handling_process import ExceptionHandlingProcess
from fairdiplomacy.selfplay.ckpt_syncer import CkptSyncer
from fairdiplomacy.selfplay.rollout import (
    ExploitRollout,
    InferencePool,
    model_output_transform_blueprint,
    model_output_transform_exploit,
    yield_rollouts,
    order_logits_to_action_logprobs,
)
from fairdiplomacy.utils.timing_ctx import TimingCtx
import heyhi

# Save regular checkpoints.
CKPT_DIR = pathlib.Path("ckpt")
CKPT_TPL = "epoch%06d.ckpt"

QUEUE_PUT_TIMEOUT = 1.0


ScoreDict = Dict[str, float]


RolloutBatch = collections.namedtuple(
    "RolloutBatch", "power_ids, observations, rewards, actions, logprobs, done"
)


def get_ckpt_sync_dir():
    # TODO(akhti): check for multinode.
    if heyhi.is_on_slurm():
        return f"/scratch/slurm_tmpdir/{heyhi.get_slurm_job_id()}/ckpt_syncer/ckpt"
    else:
        # Use NFS. Slow, but at least don't have to clean or resolve conflicts.
        return "ckpt_syncer/ckpt"


# TODO(akhti): resolve this naming hell.
def _yield_rollouts(reward_kwargs, rollout_kwargs, output_games):
    for item in yield_rollouts(**rollout_kwargs):
        game_json = item.game_json if output_games else None
        yield (rollout_to_batch(item, **reward_kwargs), game_json)


def queue_rollouts(out_queue: mp.Queue, reward_kwargs, rollout_kwargs, output_games=False) -> None:
    """A rollout worker function to push RolloutBatch's into the queue."""
    try:
        for item in _yield_rollouts(reward_kwargs, rollout_kwargs, output_games):
            try:
                out_queue.put(item, timeout=QUEUE_PUT_TIMEOUT)
            except queue_lib.Full:
                continue
    except Exception as e:
        logging.exception("Got an exception in queue_rollouts: %s", e)
        raise


def rollout_to_batch(
    rollout: ExploitRollout, *, score_name: str = None, delay_penalty=False
) -> Tuple[RolloutBatch, ScoreDict]:
    assert score_name is not None, "score_name is required"
    N = len(rollout.actions)
    scores = fairdiplomacy.utils.game_scoring.compute_game_scores(
        rollout.power_id, rollout.game_json
    )._asdict()

    rewards = torch.zeros([N], dtype=torch.float)
    rewards[-1] = scores[score_name]
    if delay_penalty:
        rewards -= delay_penalty

    is_final = torch.zeros([N], dtype=torch.bool)
    is_final[-1] = True

    # Prepare observation to be used for training. Drop information about all
    # powers, but current.
    obs = list(rollout.observations)
    obs[4] = obs[4][:, rollout.power_id].clone()
    obs[5] = obs[5][:, rollout.power_id].clone()

    rollout_batch = RolloutBatch(
        power_ids=torch.full([N], rollout.power_id, dtype=torch.long),
        rewards=rewards,
        observations=obs,
        actions=rollout.actions,
        logprobs=rollout.logprobs,
        done=is_final,
    )
    return rollout_batch, scores


def get_default_rollout_scores() -> ScoreDict:
    scores = {k: 0 for k in fairdiplomacy.utils.game_scoring.GameScores._fields}
    scores["queue_size"] = 0
    return scores


def _join_batches(batches: Sequence[RolloutBatch]) -> RolloutBatch:
    merged = {}
    for k in RolloutBatch._fields:
        values = []
        for b in batches:
            values.append(getattr(b, k))
        if k == "observations":
            values = [torch.cat(x, 0) for x in zip(*values)]
        else:
            values = torch.cat(values, 0)
        merged[k] = values
    return RolloutBatch(**merged)


def yield_batches(
    model_path, rollout_cfg: "conf.conf_pb2.ExploitTask.Rollout"
) -> Generator[Tuple[RolloutBatch, ScoreDict], None, None]:
    """Starts rollout processes and inference servers to generate impala data.

    Yields rollout in chunks. Each chunk contains concatenated
    block_size rollouts.

    Yields tuples (RolloutBatch, metrics).

    Metrics is a dict of some end-of-rollout metrics summed over all rollouts.
    """
    if torch.cuda.device_count() == 2:
        if rollout_cfg.single_rollout_gpu:
            inference_gpus = [1]
        else:
            inference_gpus = [0, 1]
    else:
        assert torch.cuda.device_count() == 8, torch.cuda.device_count()
        if rollout_cfg.single_rollout_gpu:
            inference_gpus = [1, 2]
        else:
            inference_gpus = [1, 2, 3, 4, 5, 6, 7]
    mp.set_start_method("spawn")
    if rollout_cfg.selfplay:
        logging.info("Starting selfplay PostMan servers on gpus: %s", inference_gpus)
        exploit_inference_pool = InferencePool(
            model_path=model_path,
            gpu_ids=inference_gpus,
            ckpt_sync_path=get_ckpt_sync_dir(),
            max_batch_size=rollout_cfg.inference_batch_size,
            server_procs_per_gpu=rollout_cfg.server_procs_per_gpu,
            model_output_transform=model_output_transform_exploit,
        )
        blueprint_inference_pool = None
    else:
        if len(inference_gpus) > 1:
            exploit_gpus = inference_gpus[: len(inference_gpus) // 2]
            blueprint_gpus = inference_gpus[len(inference_gpus) // 2 :]
        else:
            exploit_gpus = blueprint_gpus = inference_gpus
        logging.info("Starting exploit PostMan servers on gpus: %s", exploit_gpus)
        exploit_inference_pool = InferencePool(
            model_path=model_path,
            gpu_ids=exploit_gpus,
            ckpt_sync_path=get_ckpt_sync_dir(),
            max_batch_size=rollout_cfg.inference_batch_size,
            server_procs_per_gpu=rollout_cfg.server_procs_per_gpu,
            model_output_transform=model_output_transform_exploit,
        )
        logging.info("Starting blueprint PostMan servers on gpus: %s", blueprint_gpus)
        blueprint_inference_pool = InferencePool(
            model_path=model_path,
            gpu_ids=blueprint_gpus,
            ckpt_sync_path=None,
            max_batch_size=rollout_cfg.inference_batch_size,
            server_procs_per_gpu=rollout_cfg.server_procs_per_gpu,
            model_output_transform=model_output_transform_blueprint,
        )
    game_json = Game().to_saved_game_format()

    def _build_rollout_kwargs(proc_id):
        return dict(
            reward_kwargs=MessageToDict(rollout_cfg.reward, preserving_proto_field_name=True),
            rollout_kwargs=dict(
                initial_power_index=(proc_id % len(POWERS)),
                blueprint_hostports=blueprint_inference_pool
                and blueprint_inference_pool.hostports,
                exploit_hostports=exploit_inference_pool.hostports,
                game_json=game_json,
                max_rollout_length=rollout_cfg.rollout_max_length,
                batch_size=rollout_cfg.rollout_batch_size,
            ),
            output_games=rollout_cfg.dump_games_every > 0,
        )

    if rollout_cfg.num_rollout_processes > 0:
        logging.info("Creating rollout queue")
        queue = mp.Queue(maxsize=40)
        logging.info("Creating rollout workers")
        procs = [
            ExceptionHandlingProcess(
                target=queue_rollouts, args=[queue], kwargs=_build_rollout_kwargs(i)
            )
            for i in range(rollout_cfg.num_rollout_processes)
        ]
        logging.info("Starting rollout workers")
        for p in procs:
            p.start()
        logging.info("Done")
        rollout_generator = (queue.get() for _ in itertools.count())
    else:
        queue = None
        rollout_generator = _yield_rollouts(**_build_rollout_kwargs(0))
    rollout_iterator = iter(rollout_generator)

    accumulated_batches = []
    aggregated_scores = get_default_rollout_scores()
    size = 0
    batch_size = rollout_cfg.batch_size
    for rollout_id in itertools.count():
        rollout: RolloutBatch
        scores: ScoreDict
        # (rollout, scores), game_json = queue.get()
        (rollout, scores), game_json = next(rollout_iterator)
        if rollout_cfg.dump_games_every and rollout_id % rollout_cfg.dump_games_every == 0:
            power_id = rollout.power_ids[0].item()
            game_dump_folder = pathlib.Path(f"dumped_games")
            game_dump_folder.mkdir(exist_ok=True, parents=True)
            dump_path = game_dump_folder / f"game.{POWERS[power_id]}.{rollout_id:06d}.json"
            with (dump_path).open("w") as stream:
                json.dump(game_json, stream)

        size += len(rollout.rewards)
        accumulated_batches.append(rollout)
        for k, v in scores.items():
            aggregated_scores[k] += v
        if rollout_cfg.do_not_split_rollouts:
            if size >= batch_size:
                yield _join_batches(accumulated_batches), aggregated_scores
                # Reset.
                accumulated_batches = []
                aggregated_scores = get_default_rollout_scores()
                size = 0
        elif size > batch_size:
            # Use strict > to simplify the code.
            joined_batch = _join_batches(accumulated_batches)
            while size > batch_size:
                extracted_batch = fairdiplomacy.selfplay.metrics.rec_map(
                    lambda x: x[:batch_size], joined_batch
                )
                joined_batch = fairdiplomacy.selfplay.metrics.rec_map(
                    lambda x: x[batch_size - rollout_cfg.batch_interleave_size :], joined_batch
                )
                size -= batch_size - rollout_cfg.batch_interleave_size
                if queue is not None:
                    # Hack. We want queue size to be the size of the queue when batch is produced.
                    aggregated_scores["queue_size"] = (
                        queue.qsize() * aggregated_scores["num_games"]
                    )
                yield extracted_batch, aggregated_scores
                # Reset.
                aggregated_scores = get_default_rollout_scores()
            accumulated_batches = [joined_batch]


def compute_entropy_loss(logits, mask):
    """Return the entropy loss, i.e., the negative entropy of the policy."""
    policy = torch.nn.functional.softmax(logits, dim=-1)
    log_policy = torch.nn.functional.log_softmax(logits, dim=-1)
    return torch.sum(policy * log_policy * mask.unsqueeze(-1)) / (mask.sum() + 1e-5)


def compute_policy_gradient_loss(action_logprobs, advantages):
    return -torch.mean(action_logprobs * advantages.detach())


def to_onehot(indices, max_value):
    y_onehot = torch.zeros((len(indices), max_value), device=indices.device)
    y_onehot.scatter_(1, indices.unsqueeze(1), 1)
    return y_onehot


def build_optimizer(net, optimizer_cfg):
    return torch.optim.Adam(net.parameters(), lr=optimizer_cfg.lr)


def batched_index_select(input, dim, index):
    """Batched version of index_select with one dimensional index.

    output[b, ..pre.., ..post..] := input[b, ..pre.., index[b], ..post].
    """
    assert list(index.shape) == [input.shape[0]], (index.shape, input.shape)
    for _ in range(len(input.shape) - 1):
        index = index.unsqueeze(-1)
    expanse = list(input.shape)
    expanse[0] = -1
    expanse[dim] = -1
    index = index.expand(expanse)
    return torch.gather(input, dim, index).squeeze(dim)


def vtrace_from_logprobs_no_batch(**kwargs):
    kwargs = {k: (v.unsqueeze(1) if v.shape else v.unsqueeze(0)) for k, v in kwargs.items()}
    ret = fairdiplomacy.selfplay.vtrace.from_importance_weights(**kwargs)
    return type(ret)(*[i.squeeze(1) for i in ret])


class Logger:
    def __init__(self):
        # TODO(akhti): rank 0!
        self.writer = torch.utils.tensorboard.SummaryWriter(log_dir="tb")
        self.jsonl_writer = open("metrics.jsonl", "a")

    def log_metrics(self, metrics, step):
        for key, value in metrics.items():
            self.writer.add_scalar(key, value, global_step=step)
        created_at = datetime.datetime.utcnow().isoformat()
        print(
            json.dumps(dict(epoch=step, created_at=created_at, **metrics)),
            file=self.jsonl_writer,
            flush=True,
        )

    def __del__(self):
        if self.writer is not None:
            self.writer.close()
            self.writer = None
        if self.jsonl_writer is not None:
            self.jsonl_writer.close()
            self.jsonl_writer = None


class ExploitTrainer(object):
    def __init__(self, cfg: "conf.conf_pb2.ExploitTask"):
        self.cfg = cfg
        net = load_dipnet_model(cfg.model_path, map_location="cuda", eval=True)
        optim = build_optimizer(net, cfg.optimizer)
        self.state = argparse.Namespace(
            model=net,
            optimizer=optim,
            epoch_id=0,
            global_step=0,
            args=torch.load(cfg.model_path)["args"],
        )

        # TODO(akhti): handle multi machine.
        CKPT_DIR.mkdir(exist_ok=True, parents=True)
        if list(CKPT_DIR.iterdir()):
            *_, last_ckpt = sorted(CKPT_DIR.iterdir())
            logging.info("Found existing checkpoint folder. Will load last one: %s", last_ckpt)
            self.load_state(last_ckpt)

    def save_state(self, path):
        serialized_state = {}
        for k, v in vars(self.state).items():
            if hasattr(v, "state_dict"):
                serialized_state[k] = v.state_dict()
            else:
                serialized_state[k] = v
        torch.save(serialized_state, path)

    def load_state(self, path):
        serialized_state = torch.load(path)
        if frozenset(vars(self.state)) == frozenset(serialized_state):
            logging.error(
                "Loading state from '%s' that has different set of keys.\n\tState keys: %s\n\tckpt keys:%s",
                path,
                sorted(vars(self.state)),
                sorted(serialized_state),
            )
            raise ValueError("Bad checkpoint")
        for k, v in serialized_state.items():
            if hasattr(getattr(self.state, k), "load_state_dict"):
                getattr(self.state, k).load_state_dict(v)
            else:
                setattr(self.state, k, v)

    def run(self):
        cfg = self.cfg

        # TODO(akhti): wipe the folder.
        # TODO(akhti): handle multi machine.
        ckpt_syncer = CkptSyncer(get_ckpt_sync_dir(), create_dir=True)
        ckpt_syncer.save_state_dict(self.state.model)

        logger = Logger()
        data_loader = iter(yield_batches(cfg.model_path, cfg.rollout))
        self.state.model.train()
        max_epochs = self.cfg.trainer.max_epochs or 10 ** 9
        device = "cuda"  # Train device.
        self.state.model.to(device)
        for self.state.epoch_id in range(self.state.epoch_id, max_epochs):
            if (
                self.cfg.trainer.save_checkpoint_every
                and self.state.epoch_id % self.cfg.trainer.save_checkpoint_every == 0
            ):
                self.save_state(CKPT_DIR / (CKPT_TPL % self.state.epoch_id))
            # Coutner accumulate different statistic over the epoch. Default
            # accumulation strategy is averaging.
            counters = collections.defaultdict(fairdiplomacy.selfplay.metrics.FractionCounter)
            counters[
                "optim/grad_max"
            ] = grad_max_counter = fairdiplomacy.selfplay.metrics.MaxCounter()
            # For LR just record it's value at the start of the epoch.
            counters["optim/lr"].update(next(iter(self.state.optimizer.param_groups))["lr"])
            epoch_start_time = time.time()
            for _ in range(self.cfg.trainer.epoch_size):
                timings = TimingCtx()
                with timings("data_gen"):
                    (
                        (power_ids, obs, rewards, actions, behavior_action_logprobs, done),
                        rollout_scores,
                    ) = next(data_loader)

                with timings("to_cuda"):
                    actions = actions.to(device)
                    rewards = rewards.to(device)
                    power_ids = power_ids.to(device)
                    *obs, cand_actions = [x.to(device) for x in obs]
                    behavior_action_logprobs = behavior_action_logprobs.to(device)
                    done = done.to(device)

                with timings("net"):
                    # Shape: _, [B, 17], [B, S, 469], [B, 7].
                    # policy_cand_actions has the same information as actions,
                    # but uses local indices to match policy logits.
                    assert EOS_IDX == -1, "Rewrite the code to remove the assumption"
                    _, _, policy_logits, sc_values = self.state.model(
                        *obs,
                        temperature=1.0,
                        teacher_force_orders=actions.clamp(0),  # EOS_IDX = -1 -> 0
                        x_power=to_onehot(power_ids, len(POWERS)),
                    )
                    cand_actions = cand_actions[:, :policy_logits.shape[1]]
                    # Shape: [B].
                    sc_values = sc_values.gather(1, power_ids.unsqueeze(1)).squeeze(1)

                    # Removing absolute order ids to not use them by accident.
                    # Will use relative order ids (cand_actions) from now on.
                    del actions

                    if self.cfg.rollout.do_not_split_rollouts:
                        # Asssumes that episode actually ends.
                        bootstrap_value = torch.zeros_like(sc_values[-1])
                    else:
                        # Reducing batch size by one. Deleting things that are
                        # too lazy to adjsut to avoid artifacts.
                        bootstrap_value = sc_values[-1].detach()
                        sc_values = sc_values[:-1]
                        cand_actions = cand_actions[:-1]
                        policy_logits = policy_logits[:-1]
                        rewards = rewards[:-1]
                        del obs
                        del power_ids
                        behavior_action_logprobs = behavior_action_logprobs[:-1]
                        done = done[:-1]

                    # Shape: [B].
                    discounts = (~done).float() * self.cfg.discounting

                    # Shape: [B, 17].
                    mask = (cand_actions != EOS_IDX).float()

                    policy_action_logprobs = order_logits_to_action_logprobs(
                        policy_logits, cand_actions, mask
                    )

                    vtrace_returns = vtrace_from_logprobs_no_batch(
                        log_rhos=policy_action_logprobs - behavior_action_logprobs,
                        discounts=discounts,
                        rewards=rewards,
                        values=sc_values,
                        bootstrap_value=bootstrap_value,
                    )

                    critic_mses = 0.5 * ((vtrace_returns.vs.detach() - sc_values) ** 2)

                    losses = dict(
                        actor=compute_policy_gradient_loss(
                            policy_action_logprobs, vtrace_returns.pg_advantages
                        ),
                        critic=critic_mses.mean(),
                        # TODO(akhti): it's incorrect to apply this to
                        # per-position order distribution instead of action
                        # distribution.
                        entropy=compute_entropy_loss(policy_logits, mask),
                    )

                    loss = (
                        losses["actor"]
                        + cfg.critic_weight * losses["critic"]
                        + cfg.entropy_weight * losses["entropy"]
                    )

                    self.state.optimizer.zero_grad()
                    loss.backward()

                    if self.cfg.optimizer.grad_clip > 1e-10:
                        g_norm = torch.nn.utils.clip_grad_norm_(
                            self.state.model.parameters(), self.cfg.optimizer.grad_clip
                        )
                        grad_max_counter.update(g_norm)
                        counters["optim/grad_mean"].update(g_norm)
                        counters["optim/grad_clip_ratio"].update(
                            int(g_norm >= self.cfg.optimizer.grad_clip - 1e-5)
                        )

                    self.state.optimizer.step()
                    # Sync to make sure timing is correct.
                    loss.item()

                with timings("metrics"), torch.no_grad():
                    last_count = done.long().sum()
                    critic_end_mses = critic_mses[done].sum()

                    for key, value in losses.items():
                        counters[f"loss/{key}"].update(value)
                    counters[f"loss/total"].update(loss.item())
                    for key, value in rollout_scores.items():
                        if key != "num_games":
                            counters[f"score/{key}"].update(value, rollout_scores["num_games"])

                    counters["loss/critic_last"].update(critic_end_mses, last_count)

                    counters["reward/mean"].update(rewards.sum(), len(rewards))
                    # Rewards at the end of episodes.
                    last_sum = rewards[done].sum()
                    counters["reward/last"].update(last_sum, last_count)
                    # To match entropy loss we don't negate logprobs. So this
                    # is an estimate of the negative entropy.
                    counters["loss/entropy_sampled"].update(policy_action_logprobs.mean())

                    # Measure off-policiness.
                    counters["loss/rho"].update(
                        vtrace_returns.rhos.sum(), vtrace_returns.rhos.numel()
                    )
                    counters["loss/rhos_clipped"].update(
                        vtrace_returns.clipped_rhos.sum(), vtrace_returns.clipped_rhos.numel()
                    )

                    bsz = len(rewards)
                    counters["size/batch"].update(bsz)
                    counters["size/episode"].update(bsz, last_count)

                with timings("sync"), torch.no_grad():
                    ckpt_syncer.save_state_dict(self.state.model)

                # Doing outside of the context to capture the context's timing.
                for key, value in timings.items():
                    counters[f"time/{key}"].update(value)

                if (
                    self.state.global_step < 128
                    or (self.state.global_step & self.state.global_step + 1) == 0
                ):
                    logging.info(
                        "Metrics (global_step=%d): %s",
                        self.state.global_step,
                        {k: v.value() for k, v in sorted(counters.items())},
                    )
                self.state.global_step += 1

            epoch_scalars = {k: v.value() for k, v in sorted(counters.items())}
            average_batch_size = epoch_scalars["size/batch"]
            epoch_scalars["speed/loop_bps"] = self.cfg.trainer.epoch_size / (
                time.time() - epoch_start_time + 1e-5
            )
            epoch_scalars["speed/loop_eps"] = epoch_scalars["speed/loop_bps"] * average_batch_size
            # Speed for to_cuda + forward + backward.
            torch_time = epoch_scalars["time/net"] + epoch_scalars["time/to_cuda"]
            epoch_scalars["speed/train_bps"] = 1.0 / torch_time
            epoch_scalars["speed/train_eps"] = average_batch_size / torch_time
            logging.info("Finished epoch %d. Metrics: %s", self.state.epoch_id, epoch_scalars)
            logger.log_metrics(epoch_scalars, self.state.epoch_id)
        logging.info("Exiting main funcion")


def resolve_and_flatten(nested_dict_of_scalar_tensors):
    return fairdiplomacy.selfplay.metrics.flatten_dict(
        fairdiplomacy.selfplay.metrics.recursive_tensor_item(nested_dict_of_scalar_tensors)
    )


def task(cfg):
    ExploitTrainer(cfg).run()
