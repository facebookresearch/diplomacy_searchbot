from typing import Dict, Generator, List, Tuple, Sequence
import json
import itertools
import pathlib
import logging

import torch.multiprocessing as mp
import torch

import diplomacy
from diplomacy.utils.export import to_saved_game_format, from_saved_game_format
from fairdiplomacy.models.consts import POWERS, N_SCS
from fairdiplomacy.models.dipnet.load_model import load_dipnet_model
from fairdiplomacy.models.dipnet.order_vocabulary import EOS_IDX
from fairdiplomacy.utils.exception_handling_process import ExceptionHandlingProcess
from fairdiplomacy.selfplay.ckpt_syncer import CkptSyncer
from fairdiplomacy.selfplay.rollout import InferencePool, yield_rollouts
from fairdiplomacy.utils.timing_ctx import TimingCtx

CKPT_SYNCER_PREFIX = "ckpt_syncer/ckpt"


def queue_rollouts(
    out_queue: mp.Queue, barrier: mp.Barrier, block_size: int, **rollout_kwargs
) -> None:
    """A rollout worker function that syncs with outher workers and then produces N rollouts."""
    generator = iter(yield_rollouts(**rollout_kwargs))
    while True:
        # The trainer will call wait() when new model is loaded and rollout workers can start next iteration.
        barrier.wait()
        for item in itertools.islice(generator, block_size):
            out_queue.put(item)


def compute_rewards(power_id: int, game_jsons: Sequence[str]) -> torch.Tensor:
    """Compute N rewards given N + 1 games."""
    scs = []
    for game_json in game_jsons:
        game = from_saved_game_format(game_json)
        centers = game.get_state()["centers"]
        scs.append([len(centers[p]) for p in POWERS])
    # Shape: N x 7.
    scs = torch.tensor(scs, dtype=torch.float32)[1:]
    scs = scs[:, power_id]
    scs /= N_SCS
    return scs


def yield_batches(model_path, num_rollout_processes, block_size, rollout_kwargs, dump_games):
    """Starts rollout processes and inference servers to generate data.

    Yields rollout in chunks. Each chunk contains concatenated
    num_rollout_processes * block_size prollouts. It's guaranteed that the
    rollout proccesses are not active between then calls of this functions.

    Yields tuples of tensors:
        batch_power_ids
        batch_observations
        batch_rewards
        batch_actions
        batch_is_final
    """
    inference_gpus = [0, 1]
    inference_pool = InferencePool(
        model_path=model_path, gpu_ids=inference_gpus, ckpt_sync_path=CKPT_SYNCER_PREFIX
    )
    game_json = to_saved_game_format(diplomacy.Game())

    queue = mp.Queue()
    barrier = mp.Barrier(num_rollout_processes + 1)  # +1 = current proc.
    procs = [
        ExceptionHandlingProcess(
            target=queue_rollouts,
            args=[queue, barrier, block_size],
            kwargs=dict(
                initial_power_index=(i % len(POWERS)),
                blueprint_hostports=inference_pool.blueprint_agent_hostports,
                exploit_hostports=inference_pool.exploit_agent_hostports,
                game_json=game_json,
                **rollout_kwargs,
            ),
        )
        for i in range(num_rollout_processes)
    ]
    for p in procs:
        p.start()

    for step_id in itertools.count():
        # Allow rollout workers to start collecting rollouts.
        barrier.wait()
        batch_power_ids, batch_observations, batch_rewards, batch_actions, batch_is_final = (
            [],
            [],
            [],
            [],
            [],
        )
        for batch_id in range(num_rollout_processes * block_size):
            power_id: int
            game_jsons: List[str]
            actions: torch.Tensor
            observations: List[torch.Tensor]
            power_id, game_jsons, actions, observations = queue.get()
            N = len(actions)
            assert len(game_jsons) == N + 1, (
                len(game_jsons),
                [x.shape for x in observations],
                actions.shape,
            )
            if dump_games and batch_id == 0:
                game_dump_folder = pathlib.Path(f"dumped_games")
                game_dump_folder.mkdir(exist_ok=True, parents=True)
                with (game_dump_folder / f"game{step_id:06d}.json").open("w") as stream:
                    json.dump(game_jsons[-1], stream)

            batch_power_ids.append(torch.full([N], power_id, dtype=torch.long))
            batch_rewards.append(compute_rewards(power_id, game_jsons))
            batch_observations.append(observations)
            batch_actions.append(actions)
            is_final = torch.zeros([N], dtype=torch.bool)
            is_final[-1] = True
            batch_is_final.append(is_final)
        batch_power_ids = torch.cat(batch_power_ids, 0)
        batch_observations = [torch.cat(x, 0) for x in zip(*batch_observations)]
        batch_rewards = torch.cat(batch_rewards, 0)
        batch_actions = torch.cat(batch_actions, 0)
        batch_is_final = torch.cat(batch_is_final, 0)
        yield batch_power_ids, batch_observations, batch_rewards, batch_actions, batch_is_final


def compute_action_loss(advantages, order_distributions, actions, order_mask):
    advantages = advantages.detach()
    # Shape: [B, S].
    order_log_probs = order_distributions.log_prob(actions)
    # Shape: [B].
    action_log_probs = (order_log_probs * order_mask).sum(-1)
    action_loss = -advantages * action_log_probs
    return action_loss.mean()


def compute_value_loss(advantages):
    value_loss = 0.5 * advantages.pow(2)
    return value_loss.mean()


def compute_entropy_loss(distribution, mask, dones=None):
    entropy_loss = -distribution.entropy()
    entropy_loss = entropy_loss * mask
    return entropy_loss.mean()


def to_onehot(indices, max_value):
    y_onehot = torch.zeros((len(indices), max_value), device=indices.device)
    y_onehot.scatter_(1, indices.unsqueeze(1), 1)
    return y_onehot


def build_optimizer(net, optimizer_cfg):
    return torch.optim.Adam(net.parameters(), lr=optimizer_cfg.lr)


def batched_index_select(input, dim, index):
    """Batched version of index_select with one dimensional index.

    output[b, ..pre.., ..post..] := input[b, ..pre.., index[b], ..post].
    """
    assert list(index.shape) == [input.shape[0]], (index.shape, input.shape)
    for _ in range(len(input.shape) - 1):
        index = index.unsqueeze(-1)
    expanse = list(input.shape)
    expanse[0] = -1
    expanse[dim] = -1
    index = index.expand(expanse)
    return torch.gather(input, dim, index).squeeze(dim)


def recursive_tensor_item(tensor_nest):
    if isinstance(tensor_nest, torch.Tensor):
        return tensor_nest.detach().cpu().item()
    if isinstance(tensor_nest, (tuple, list)):
        return type(tensor_nest)(map(recursive_tensor_item, tensor_nest))
    if isinstance(tensor_nest, dict):
        return type(tensor_nest)((k, recursive_tensor_item(v)) for k, v in tensor_nest.items())
    return tensor_nest


def flatten_dict(tensor_dict):
    if not isinstance(tensor_dict, dict):
        return tensor_dict
    dd = {}
    for k, v in tensor_dict.items():
        v = flatten_dict(v)
        if isinstance(v, dict):
            for subkey, subvaule in v.items():
                dd[f"{k}/{subkey}"] = subvaule
        else:
            dd[k] = v
    dd = dict(sorted(dd.items()))
    return dd


def task(cfg):
    net = load_dipnet_model(cfg.model_path, map_location="cuda", eval=True)

    ckpt_syncer = CkptSyncer(CKPT_SYNCER_PREFIX, create_dir=True)
    optim = build_optimizer(net, cfg.optimizer)

    # Number of the rollout processes.
    num_rollout_processes = cfg.rollout.num_rollout_processes
    rollout_kwargs = dict(
        max_rollout_length=cfg.rollout.max_rollout_length, batch_size=cfg.rollout.batch_size
    )
    # Each procces makes a single rollout for a single power.
    block_size = cfg.rollout.block_size * rollout_kwargs["batch_size"]
    ckpt_syncer.save_state_dict(net)
    data_loader = iter(
        yield_batches(
            cfg.model_path,
            num_rollout_processes,
            block_size,
            rollout_kwargs,
            cfg.rollout.dump_games,
        )
    )
    timings = TimingCtx()
    net.train()
    for iter_id in itertools.count():
        with timings("data_gen"):
            power_ids, obs, rewards, actions, is_final = next(data_loader)
        with timings("to_cuda"):
            actions = actions.cuda()
            rewards = rewards.cuda()
            power_ids = power_ids.cuda()
            obs = [x.cuda() for x in obs]

        with timings("net"):
            # TODO(akhti): move up state?
            # As we are using x_power, we have to remove input information for
            # other powers.
            obs[4] = batched_index_select(obs[4], 1, power_ids)
            obs[5] = batched_index_select(obs[5], 1, power_ids)

            # Shape: _, [B, 17, 13k], [B, 7].
            _, order_logits, sc_values = net(
                *obs,
                temperature=1.0,
                teacher_force_orders=actions,
                x_power=to_onehot(power_ids, len(POWERS)),
            )
            # Shape: [B].
            sc_values = sc_values.gather(1, power_ids.unsqueeze(1))

            advantages = rewards - sc_values / N_SCS

            order_distributions = torch.distributions.Categorical(logits=order_logits)
            # Shape: [B, 17].
            mask = (actions != EOS_IDX).float()
            losses = dict(
                actor=compute_action_loss(advantages, order_distributions, actions, mask),
                critic=compute_value_loss(advantages),
                entropy=compute_entropy_loss(order_distributions, mask),
            )
            loss = (
                losses["actor"]
                + cfg.critic_weight * losses["critic"]
                + cfg.entropy_weight * losses["entropy"]
            )

            optim.zero_grad()
            loss.backward()
            optim.step()

        losses = losses.copy()
        losses["total"] = loss
        istant_metrics = flatten_dict(
            recursive_tensor_item(
                {
                    "loss": losses,
                    "times": dict(timings.items()),
                    "batch_size": len(actions),
                    "reward": {"mean": rewards.mean(), "final": rewards[is_final].mean()},
                }
            )
        )
        logging.info("Metrics (iter=%d): %s", iter_id, istant_metrics)
        ckpt_syncer.save_state_dict(net)
