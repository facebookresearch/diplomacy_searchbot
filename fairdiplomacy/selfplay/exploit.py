from typing import Dict, Generator, List, Tuple, Sequence
import json
import itertools
import pathlib
import logging

import torch.multiprocessing as mp
import torch

import diplomacy
from diplomacy.utils.export import to_saved_game_format, from_saved_game_format
import fairdiplomacy.selfplay.vtrace
from fairdiplomacy.models.consts import POWERS, N_SCS
from fairdiplomacy.models.dipnet.load_model import load_dipnet_model
from fairdiplomacy.models.dipnet.order_vocabulary import EOS_IDX
from fairdiplomacy.utils.exception_handling_process import ExceptionHandlingProcess
from fairdiplomacy.selfplay.ckpt_syncer import CkptSyncer
from fairdiplomacy.selfplay.rollout import ExploitRollout, InferencePool, yield_rollouts
from fairdiplomacy.utils.timing_ctx import TimingCtx

CKPT_SYNCER_PREFIX = "ckpt_syncer/ckpt"


def queue_rollouts(out_queue: mp.Queue, **rollout_kwargs) -> None:
    """A rollout worker function that syncs with outher workers and then produces N rollouts."""
    for item in yield_rollouts(**rollout_kwargs):
        out_queue.put(item)


def compute_rewards(power_id: int, game_jsons: Sequence[str]) -> torch.Tensor:
    """Compute N rewards given N + 1 games."""
    rewards = torch.zeros([len(game_jsons) - 1])
    game = from_saved_game_format(game_jsons[-1])
    centers = game.get_state()["centers"]
    rewards[-1] = len(centers[POWERS[power_id]]) / N_SCS
    return rewards


def yield_batches(model_path, num_rollout_processes, block_size, rollout_kwargs, dump_games):
    """Starts rollout processes and inference servers to generate impala data.

    Yields rollout in chunks. Each chunk contains concatenated
    block_size rollouts.

    Yields tuples of tensors:
        batch_power_ids
        batch_observations
        batch_rewards
        batch_actions
        batch_logprobs
        batch_is_done
    """
    inference_gpus = [0, 1]
    inference_pool = InferencePool(
        model_path=model_path, gpu_ids=inference_gpus, ckpt_sync_path=CKPT_SYNCER_PREFIX
    )
    game_json = to_saved_game_format(diplomacy.Game())

    queue = mp.Queue()
    procs = [
        ExceptionHandlingProcess(
            target=queue_rollouts,
            args=[queue],
            kwargs=dict(
                initial_power_index=(i % len(POWERS)),
                blueprint_hostports=inference_pool.blueprint_agent_hostports,
                exploit_hostports=inference_pool.exploit_agent_hostports,
                game_json=game_json,
                **rollout_kwargs,
            ),
        )
        for i in range(num_rollout_processes)
    ]
    for p in procs:
        p.start()

    for step_id in itertools.count():
        batch = {}
        batch["power_ids"] = []
        batch["observations"] = []
        batch["rewards"] = []
        batch["actions"] = []
        batch["logprobs"] = []
        batch["done"] = []
        for batch_id in range(block_size):
            rollout: ExploitRollout = queue.get()
            N = len(rollout.actions)
            assert len(rollout.game_jsons) == N + 1, (
                len(rollout.game_jsons),
                [x.shape for x in rollout.observations],
                rollout.actions.shape,
            )
            if dump_games and batch_id == 0:
                game_dump_folder = pathlib.Path(f"dumped_games")
                game_dump_folder.mkdir(exist_ok=True, parents=True)
                with (game_dump_folder / f"game{step_id:06d}.json").open("w") as stream:
                    json.dump(rollout.game_jsons[-1], stream)

            batch["power_ids"].append(torch.full([N], rollout.power_id, dtype=torch.long))
            batch["rewards"].append(compute_rewards(rollout.power_id, rollout.game_jsons))
            batch["observations"].append(rollout.observations)
            batch["actions"].append(rollout.actions)
            batch["logprobs"].append(rollout.logprobs)
            is_final = torch.zeros([N], dtype=torch.bool)
            is_final[-1] = True
            batch["done"].append(is_final)
        for k in list(batch):
            if k == "observations":
                batch[k] = [torch.cat(x, 0) for x in zip(*batch[k])]
            else:
                batch[k] = torch.cat(batch[k], 0)
        yield tuple(batch.values())


def compute_baseline_loss(advantages):
    return 0.5 * (advantages ** 2).mean()


def compute_entropy_loss(logits, mask):
    """Return the entropy loss, i.e., the negative entropy of the policy."""
    policy = torch.nn.functional.softmax(logits, dim=-1)
    log_policy = torch.nn.functional.log_softmax(logits, dim=-1)
    return torch.sum(policy * log_policy * mask.unsqueeze(-1)) / (mask.sum() + 1e-5)


def compute_policy_gradient_loss(logits, actions, advantages, mask):
    cross_entropy = torch.nn.functional.nll_loss(
        torch.nn.functional.log_softmax(torch.flatten(logits, end_dim=-2), dim=-1),
        target=torch.flatten(actions),
        reduction="none",
    )
    cross_entropy = (cross_entropy.view_as(actions) * mask).sum(-1)
    return torch.mean(cross_entropy * advantages.detach())


def to_onehot(indices, max_value):
    y_onehot = torch.zeros((len(indices), max_value), device=indices.device)
    y_onehot.scatter_(1, indices.unsqueeze(1), 1)
    return y_onehot


def build_optimizer(net, optimizer_cfg):
    return torch.optim.Adam(net.parameters(), lr=optimizer_cfg.lr)


def batched_index_select(input, dim, index):
    """Batched version of index_select with one dimensional index.

    output[b, ..pre.., ..post..] := input[b, ..pre.., index[b], ..post].
    """
    assert list(index.shape) == [input.shape[0]], (index.shape, input.shape)
    for _ in range(len(input.shape) - 1):
        index = index.unsqueeze(-1)
    expanse = list(input.shape)
    expanse[0] = -1
    expanse[dim] = -1
    index = index.expand(expanse)
    return torch.gather(input, dim, index).squeeze(dim)


def recursive_tensor_item(tensor_nest):
    if isinstance(tensor_nest, torch.Tensor):
        return tensor_nest.detach().cpu().item()
    if isinstance(tensor_nest, (tuple, list)):
        return type(tensor_nest)(map(recursive_tensor_item, tensor_nest))
    if isinstance(tensor_nest, dict):
        return type(tensor_nest)((k, recursive_tensor_item(v)) for k, v in tensor_nest.items())
    return tensor_nest


def flatten_dict(tensor_dict):
    if not isinstance(tensor_dict, dict):
        return tensor_dict
    dd = {}
    for k, v in tensor_dict.items():
        v = flatten_dict(v)
        if isinstance(v, dict):
            for subkey, subvaule in v.items():
                dd[f"{k}/{subkey}"] = subvaule
        else:
            dd[k] = v
    dd = dict(sorted(dd.items()))
    return dd


def vtrace_from_logits_no_batch(**kwargs):
    kwargs = {k: (v.unsqueeze(1) if v.shape else v.unsqueeze(0)) for k, v in kwargs.items()}
    ret = fairdiplomacy.selfplay.vtrace.from_logits(**kwargs)
    return type(ret)(*[i.squeeze(1) for i in ret])


def vtrace_trainer(cfg):
    discounting = 0.99

    net = load_dipnet_model(cfg.model_path, map_location="cuda", eval=True)

    ckpt_syncer = CkptSyncer(CKPT_SYNCER_PREFIX, create_dir=True)
    optim = build_optimizer(net, cfg.optimizer)

    # Number of the rollout processes.
    num_rollout_processes = cfg.rollout.num_rollout_processes
    rollout_kwargs = dict(
        max_rollout_length=cfg.rollout.max_rollout_length, batch_size=cfg.rollout.batch_size
    )
    # Each procces makes a single rollout for a single power.
    block_size = cfg.rollout.block_size * rollout_kwargs["batch_size"]
    ckpt_syncer.save_state_dict(net)
    data_loader = iter(
        yield_batches(
            cfg.model_path,
            num_rollout_processes,
            block_size,
            rollout_kwargs,
            cfg.rollout.dump_games,
        )
    )
    timings = TimingCtx()
    net.train()
    for iter_id in itertools.count():
        with timings("data_gen"):
            power_ids, obs, rewards, actions, behavior_logprobs, done = next(data_loader)
        with timings("to_cuda"):
            actions = actions.cuda()
            rewards = rewards.cuda()
            power_ids = power_ids.cuda()
            obs = [x.cuda() for x in obs]
            behavior_logprobs = behavior_logprobs.cuda()
            done = done.cuda()

        with timings("net"):
            # TODO(akhti): move up state?
            # As we are using x_power, we have to remove input information for
            # other powers.
            obs[4] = batched_index_select(obs[4], 1, power_ids)
            obs[5] = batched_index_select(obs[5], 1, power_ids)

            # Shape: _, [B, 17, 13k], [B, 7].
            _, policy_logits, sc_values = net(
                *obs,
                temperature=1.0,
                teacher_force_orders=actions,
                x_power=to_onehot(power_ids, len(POWERS)),
            )
            # Shape: [B].
            sc_values = sc_values.gather(1, power_ids.unsqueeze(1)).squeeze(1) / N_SCS

            # Shape: [B].
            discounts = (~done).float() * discounting

            # Shape: [B, 17].
            mask = (actions != EOS_IDX).float()

            vtrace_returns = vtrace_from_logits_no_batch(
                behavior_policy_logits=behavior_logprobs,
                target_policy_logits=policy_logits,
                action_mask=mask,
                actions=actions,
                discounts=discounts,
                rewards=rewards,
                values=sc_values,
                # Asssumes that episode actually ends..
                bootstrap_value=torch.zeros_like(sc_values[-1]),
            )

            losses = dict(
                actor=compute_policy_gradient_loss(
                    policy_logits, actions, vtrace_returns.pg_advantages, mask
                ),
                critic=compute_baseline_loss(vtrace_returns.vs - sc_values),
                entropy=compute_entropy_loss(policy_logits, mask),
            )

            loss = (
                losses["actor"]
                + cfg.critic_weight * losses["critic"]
                + cfg.entropy_weight * losses["entropy"]
            )

            optim.zero_grad()
            loss.backward()
            optim.step()

        losses = losses.copy()
        losses["total"] = loss
        istant_metrics = flatten_dict(
            recursive_tensor_item(
                {
                    "loss": losses,
                    "times": dict(timings.items()),
                    "batch_size": len(actions),
                    "reward": {"mean": rewards.mean(), "final": rewards[done].mean()},
                }
            )
        )
        logging.info("Metrics (iter=%d): %s", iter_id, istant_metrics)
        ckpt_syncer.save_state_dict(net)


def task(cfg):
    return vtrace_trainer(cfg)
