from typing import Dict, Generator, List, Tuple, Sequence
import itertools
import pathlib
import logging

import torch.multiprocessing as mp
import torch

import diplomacy
from diplomacy.utils.export import to_saved_game_format, from_saved_game_format
from fairdiplomacy.models.consts import POWERS, N_SCS
from fairdiplomacy.models.dipnet.load_model import load_dipnet_model
from fairdiplomacy.models.dipnet.order_vocabulary import EOS_IDX
from fairdiplomacy.utils.exception_handling_process import ExceptionHandlingProcess
from fairdiplomacy.selfplay.rollout import InferencePool, yield_rollouts
from fairdiplomacy.selfplay.ckpt_syncer import CkptSyncer

CKPT_SYNCER_PREFIX = "ckpt_syncer/ckpt"


def queue_rollouts(out_queue: mp.Queue, barrier: mp.Barrier, block_size: int, **kwargs) -> None:
    """A rollout worker function that syncs with outher workers and then produces N rollouts."""
    generator = iter(yield_rollouts(**kwargs))
    while True:
        # The trainer will call wait() when new model is loaded and rollout workers can start next iteration.
        barrier.wait()
        for item in itertools.islice(generator, block_size):
            out_queue.put(item)


def compute_rewards(power_id: int, game_jsons: Sequence[str]) -> torch.Tensor:
    """Compute N rewards given N + 1 games."""
    scs = []
    for game_json in game_jsons:
        game = from_saved_game_format(game_json)
        centers = game.get_state()["centers"]
        scs.append([len(centers[p]) for p in POWERS])
    # Shape: N x 7.
    scs = torch.tensor(scs, dtype=torch.float32)[1:]
    scs = scs[:, power_id]
    scs /= N_SCS
    return scs


def yield_batches(model_path, num_rollout_process, block_size):
    """Starts rollout processes and inference servers to generate data.

    Yields rollout in chunks. Each chunk contains concatenated
    num_rollout_process * block_size prollouts. It's guaranteed that the
    rollout proccesses are not active between then calls of this functions.

    Yields tuples of tensors:
        batch_power_ids
        batch_observations
        batch_rewards
        batch_actions
        batch_is_final
    """
    inference_gpus = [0, 1]
    inference_pool = InferencePool(
        model_path=model_path, gpu_ids=inference_gpus, ckpt_sync_path=CKPT_SYNCER_PREFIX
    )
    game_json = to_saved_game_format(diplomacy.Game())

    queue = mp.Queue()
    barrier = mp.Barrier(num_rollout_process + 1)  # +1 = current proc.
    rollout_kwargs = dict(
        blueprint_hostports=inference_pool.blueprint_agent_hostports,
        exploit_hostports=inference_pool.exploit_agent_hostports,
        game_json=game_json,
    )
    procs = [
        ExceptionHandlingProcess(
            target=queue_rollouts,
            args=[queue, barrier, block_size],
            kwargs=dict(initial_power_index=(i % len(POWERS)), **rollout_kwargs),
        )
        for i in range(num_rollout_process)
    ]
    for p in procs:
        p.start()

    while True:
        # Allow rollout workers to start collecting rollouts.
        barrier.wait()
        batch_power_ids, batch_observations, batch_rewards, batch_actions, batch_is_final = (
            [],
            [],
            [],
            [],
            [],
        )
        for _ in range(num_rollout_process * block_size):
            power_id: int
            game_jsons: List[str]
            actions: torch.Tensor
            observations: List[torch.Tensor]
            power_id, game_jsons, actions, observations = queue.get()
            N = len(actions)
            assert len(game_jsons) == N + 1, (
                len(game_jsons),
                [x.shape for x in observations],
                actions.shape,
            )
            batch_power_ids.append(torch.full([N], power_id, dtype=torch.long))
            batch_rewards.append(compute_rewards(power_id, game_jsons))
            batch_observations.append(observations)
            batch_actions.append(actions)
            is_final = torch.zeros([N], dtype=torch.bool)
            is_final[-1] = True
            batch_is_final.append(is_final)
        batch_power_ids = torch.cat(batch_power_ids, 0)
        batch_observations = [torch.cat(x, 0) for x in zip(*batch_observations)]
        batch_rewards = torch.cat(batch_rewards, 0)
        batch_actions = torch.cat(batch_actions, 0)
        batch_is_final = torch.cat(batch_is_final, 0)
        yield batch_power_ids, batch_observations, batch_rewards, batch_actions, batch_is_final


def compute_action_loss(advantages, order_distributions, actions, order_mask):
    advantages = advantages.detach()
    # Shape: [B, S].
    order_log_probs = order_distributions.log_prob(actions)
    # Shape: [B].
    action_log_probs = (order_log_probs * order_mask).sum(-1)
    action_loss = -advantages * action_log_probs
    return action_loss.mean()


def compute_value_loss(advantages):
    value_loss = 0.5 * advantages.pow(2)
    return value_loss.mean()


def compute_entropy_loss(distribution, mask, dones=None):
    entropy_loss = -distribution.entropy()
    entropy_loss = entropy_loss * mask
    return entropy_loss.mean()


def to_onehot(indices, max_value):
    y_onehot = torch.zeros((len(indices), max_value), device=indices.device)
    y_onehot.scatter_(1, indices.unsqueeze(1), 1)
    return y_onehot


def build_optimizer(net, optimizer_cfg):
    return torch.optim.Adam(net.parameters(), lr=optimizer_cfg.lr)


def batched_index_select(input, dim, index):
    """Batched version of index_select with one dimensional index.

    output[b, ..pre.., ..post..] := input[b, ..pre.., index[b], ..post].
    """
    assert list(index.shape) == [input.shape[0]], (index.shape, input.shape)
    for _ in range(len(input.shape) - 1):
        index = index.unsqueeze(-1)
    expanse = list(input.shape)
    expanse[0] = -1
    expanse[dim] = -1
    index = index.expand(expanse)
    return torch.gather(input, dim, index).squeeze(dim)


def recursive_tensor_item(tensor_nest):
    if isinstance(tensor_nest, torch.Tensor):
        return tensor_nest.detach().cpu().item()
    if isinstance(tensor_nest, (tuple, list)):
        return type(tensor_nest)(map(recursive_tensor_item, tensor_nest))
    if isinstance(tensor_nest, dict):
        return type(tensor_nest)((k, recursive_tensor_item(v)) for k, v in tensor_nest.items())
    return tensor_nest


def flatten_dict(tensor_dict):
    if not isinstance(tensor_dict, dict):
        return tensor_dict
    dd = {}
    for k, v in tensor_dict.items():
        v = flatten_dict(v)
        if isinstance(v, dict):
            for subkey, subvaule in v.items():
                dd[f"{k}/{subkey}"] = subvaule
        else:
            dd[k] = v
    dd = dict(sorted(dd.items()))
    return dd


def task(cfg):
    net = load_dipnet_model(cfg.model_path, map_location="cuda", eval=True)

    ckpt_syncer = CkptSyncer(CKPT_SYNCER_PREFIX, create_dir=True)
    optim = build_optimizer(net, cfg.optimizer)

    # Number of the rollout processes.
    num_rollout_process = 10
    # Each procces makes a single rollout for a single power.
    block_size = 1
    data_loader = yield_batches(cfg.model_path, num_rollout_process, block_size)
    ckpt_syncer.save_state_dict(net)
    for iter_id, (power_ids, obs, rewards, actions, is_final) in enumerate(data_loader):
        net.train()
        actions = actions.cuda()
        rewards = rewards.cuda()
        power_ids = power_ids.cuda()
        obs = [x.cuda() for x in obs]

        # TODO(akhti): move up state?
        # As we are using x_power, we have to remove input information for
        # other powers.
        obs[4] = batched_index_select(obs[4], 1, power_ids)
        obs[5] = batched_index_select(obs[5], 1, power_ids)

        temp = 1.0
        # Shape: _, [B, 17, 13k], [B, 7].
        _, order_logits, sc_values = net(
            *obs,
            temperature=temp,
            teacher_force_orders=actions,
            x_power=to_onehot(power_ids, len(POWERS)),
        )
        # Shape: [B].
        sc_values = sc_values.gather(1, power_ids.unsqueeze(1))

        advantages = rewards - sc_values / N_SCS

        order_distributions = torch.distributions.Categorical(logits=order_logits)
        # Shape: [B, 17].
        mask = (actions != EOS_IDX).float()
        losses = dict(
            actor=compute_action_loss(advantages, order_distributions, actions, mask),
            critic=compute_value_loss(advantages),
            entropy=compute_entropy_loss(order_distributions, mask),
        )
        loss = (
            losses["actor"]
            + cfg.critic_weight * losses["critic"]
            + cfg.entropy_weight * losses["entropy"]
        )

        optim.zero_grad()
        loss.backward()
        optim.step()

        losses = losses.copy()
        losses["total"] = loss
        istant_metrics = flatten_dict(
            recursive_tensor_item(
                {
                    "loss": losses,
                    "batch_size": len(actions),
                    "reward": {"mean": rewards.mean(), "final": rewards[is_final].mean()},
                }
            )
        )
        logging.info("Metrics (iter=%d): %s", iter_id, istant_metrics)
        ckpt_syncer.save_state_dict(net)
