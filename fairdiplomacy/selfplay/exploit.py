from typing import Dict, Generator, List, Optional, Tuple, Sequence
import argparse
import collections
import itertools
import json
import logging
import multiprocessing as mp
import pathlib
import time
import queue as queue_lib

from google.protobuf.json_format import MessageToDict
import attr
import torch
import torch.utils.tensorboard


import fairdiplomacy.selfplay.metrics
import fairdiplomacy.selfplay.vtrace
from fairdiplomacy.game import Game
from fairdiplomacy.models.consts import POWERS
from fairdiplomacy.models.dipnet.load_model import load_dipnet_model
from fairdiplomacy.models.dipnet.order_vocabulary import EOS_IDX
from fairdiplomacy.utils.exception_handling_process import ExceptionHandlingProcess
from fairdiplomacy.utils import game_scoring
from fairdiplomacy.selfplay.ckpt_syncer import CkptSyncer
from fairdiplomacy.selfplay.rollout import (
    ExploitRollout,
    InferencePool,
    RolloutMode,
    model_output_transform_blueprint,
    model_output_transform_exploit,
    yield_rollouts,
    order_logits_to_action_logprobs,
)
from fairdiplomacy.utils.timing_ctx import TimingCtx
import heyhi

# Save regular checkpoints.
CKPT_DIR = pathlib.Path("ckpt")
CKPT_TPL = "epoch%06d.ckpt"

REQUEUE_CKPT = pathlib.Path("requeue.ckpt")

QUEUE_PUT_TIMEOUT = 1.0


ScoreDict = Dict[str, float]


RolloutBatch = collections.namedtuple(
    "RolloutBatch", "power_ids, observations, rewards, actions, logprobs, done"
)


def get_ckpt_sync_dir():
    # TODO(akhti): check for multinode.
    if heyhi.is_on_slurm():
        return f"/scratch/slurm_tmpdir/{heyhi.get_slurm_job_id()}/ckpt_syncer/ckpt"
    else:
        # Use NFS. Slow, but at least don't have to clean or resolve conflicts.
        return "ckpt_syncer/ckpt"


# TODO(akhti): resolve this naming hell.
def _yield_rollouts(reward_kwargs, rollout_kwargs, output_games):
    for item in yield_rollouts(**rollout_kwargs):
        game_json = item.game_json if output_games else None
        yield (rollout_to_batch(item, **reward_kwargs), game_json)


def queue_rollouts(out_queue: mp.Queue, reward_kwargs, rollout_kwargs, output_games=False) -> None:
    """A rollout worker function to push RolloutBatch's into the queue."""
    try:
        for item in _yield_rollouts(reward_kwargs, rollout_kwargs, output_games):
            try:
                out_queue.put(item, timeout=QUEUE_PUT_TIMEOUT)
            except queue_lib.Full:
                continue
    except Exception as e:
        logging.exception("Got an exception in queue_rollouts: %s", e)
        raise


def queue_scores(out_queue: mp.Queue, reward_kwargs, rollout_kwargs) -> None:
    """A rollout worker function to push scores from eval into the queue."""
    try:
        for (_, scores), _ in _yield_rollouts(reward_kwargs, rollout_kwargs, output_games=False):
            try:
                out_queue.put(scores, timeout=QUEUE_PUT_TIMEOUT)
            except queue_lib.Full:
                continue
    except Exception as e:
        logging.exception("Got an exception in queue_rollouts: %s", e)
        raise


def rollout_to_batch(
    rollout: ExploitRollout,
    *,
    score_name: str = None,
    delay_penalty=False,
    differential_reward=False,
) -> Tuple[RolloutBatch, ScoreDict]:
    assert score_name is not None, "score_name is required"
    N = len(rollout.actions)
    scores = game_scoring.compute_game_scores(rollout.power_id, rollout.game_json)._asdict()
    if differential_reward:
        assert len(rollout.game_json["phases"]) == N + 1, (len(rollout.game_json["phases"]), N)
        target_score_history = torch.FloatTensor(
            [
                getattr(game_scoring.compute_phase_scores(rollout.power_id, phase), score_name,)
                for phase in rollout.game_json["phases"]
            ]
        )
        rewards = target_score_history[1:] - target_score_history[:-1]
    else:
        rewards = torch.zeros([N], dtype=torch.float)
        rewards[-1] = scores[score_name]

    if delay_penalty:
        rewards -= delay_penalty

    is_final = torch.zeros([N], dtype=torch.bool)
    is_final[-1] = True

    # Prepare observation to be used for training. Drop information about all
    # powers, but current.
    obs = list(rollout.observations)
    obs[4] = obs[4][:, rollout.power_id].clone()
    obs[5] = obs[5][:, rollout.power_id].clone()

    rollout_batch = RolloutBatch(
        power_ids=torch.full([N], rollout.power_id, dtype=torch.long),
        rewards=rewards,
        observations=obs,
        actions=rollout.actions,
        logprobs=rollout.logprobs,
        done=is_final,
    )
    return rollout_batch, scores


def get_default_rollout_scores() -> ScoreDict:
    scores = {k: 0 for k in fairdiplomacy.utils.game_scoring.GameScores._fields}
    scores["queue_size"] = 0
    return scores


def _join_batches(batches: Sequence[RolloutBatch]) -> RolloutBatch:
    merged = {}
    for k in RolloutBatch._fields:
        values = []
        for b in batches:
            values.append(getattr(b, k))
        if k == "observations":
            values = [torch.cat(x, 0) for x in zip(*values)]
        else:
            values = torch.cat(values, 0)
        merged[k] = values
    return RolloutBatch(**merged)


class Evaler:
    def __init__(
        self,
        *,
        model_path,
        reward_cfg,
        num_procs,
        blueprint_hostports,
        exploit_hostports,
        temperature,
        max_length=100,
    ):
        self.model_path = model_path

        game_json = Game().to_saved_game_format()

        def _build_rollout_kwargs(proc_id):
            return dict(
                reward_kwargs=MessageToDict(reward_cfg, preserving_proto_field_name=True),
                rollout_kwargs=dict(
                    mode=RolloutMode.EVAL,
                    initial_power_index=(proc_id % len(POWERS)),
                    blueprint_hostports=blueprint_hostports,
                    exploit_hostports=exploit_hostports,
                    game_json=game_json,
                    temperature=temperature,
                    max_rollout_length=max_length,
                    batch_size=1,
                ),
            )

        logging.info("Creating eval rollout queue")
        self.queue = mp.Queue(maxsize=4000)
        logging.info("Creating eval rollout workers")
        self.procs = [
            ExceptionHandlingProcess(
                target=queue_scores,
                args=[self.queue],
                kwargs=_build_rollout_kwargs(i),
                daemon=True,
            )
            for i in range(num_procs)
        ]
        logging.info("Starting eval rollout workers")
        for p in self.procs:
            p.start()
        logging.info("Done")

    def extract_scores(self) -> ScoreDict:
        qsize = self.queue.qsize()
        aggregated_scores = get_default_rollout_scores()
        for _ in range(qsize):
            try:
                scores = self.queue.get_nowait()
            except queue_lib.Empty:
                logging.warning("Kind of odd...")
                break
            for k, v in scores.items():
                aggregated_scores[k] += v
        for k in list(aggregated_scores):
            aggregated_scores[k] /= aggregated_scores["num_games"]
        del aggregated_scores["queue_size"]
        return aggregated_scores

    def terminate(self):
        logging.info("Killing eval processes")
        for proc in self.procs:
            proc.kill()


class DataLoader:
    """Starts rollout processes and inference servers to generate impala data.

    Yields rollout in chunks. Each chunk contains concatenated
    block_size rollouts.

    Yields tuples (RolloutBatch, metrics).

    Metrics is a dict of some end-of-rollout metrics summed over all rollouts.
    """

    def __init__(self, model_path, rollout_cfg: "conf.conf_pb2.ExploitTask.Rollout"):
        self.model_path = model_path
        self.rollout_cfg = rollout_cfg

        mp.set_start_method("spawn")

        self._start_inference_procs()
        self._start_rollout_procs()
        self._maybe_start_eval_procs()
        self._batch_iterator = iter(self._yield_batches())

    def _start_inference_procs(self):
        if torch.cuda.device_count() == 2:
            if self.rollout_cfg.single_rollout_gpu:
                inference_gpus = [1]
            else:
                inference_gpus = [0, 1]
        else:
            assert torch.cuda.device_count() == 8, torch.cuda.device_count()
            if self.rollout_cfg.single_rollout_gpu:
                inference_gpus = [1, 2]
            else:
                inference_gpus = [1, 2, 3, 4, 5, 6, 7]

        if len(inference_gpus) == 1:
            exploit_gpus = blueprint_gpus = inference_gpus
        elif self.rollout_cfg.selfplay:
            # Assign only one GPU for eval.
            blueprint_gpus = inference_gpus[:1]
            exploit_gpus = inference_gpus[1:]
        else:
            exploit_gpus = inference_gpus[: len(inference_gpus) // 2]
            blueprint_gpus = inference_gpus[len(inference_gpus) // 2 :]

        logging.info("Starting exploit PostMan servers on gpus: %s", exploit_gpus)
        exploit_inference_pool = InferencePool(
            model_path=self.model_path,
            gpu_ids=exploit_gpus,
            ckpt_sync_path=get_ckpt_sync_dir(),
            ckpt_sync_every=self.rollout_cfg.inference_ckpt_sync_every,
            max_batch_size=self.rollout_cfg.inference_batch_size,
            server_procs_per_gpu=self.rollout_cfg.server_procs_per_gpu,
            model_output_transform=model_output_transform_exploit,
        )
        logging.info("Starting blueprint PostMan servers on gpus: %s", blueprint_gpus)
        blueprint_inference_pool = InferencePool(
            model_path=self.model_path,
            gpu_ids=blueprint_gpus,
            ckpt_sync_path=None,
            max_batch_size=self.rollout_cfg.inference_batch_size,
            server_procs_per_gpu=self.rollout_cfg.server_procs_per_gpu,
            model_output_transform=model_output_transform_blueprint,
        )

        # Must save reference to the pools to keep the resources alive.
        self.blueprint_inference_pool = blueprint_inference_pool
        self.exploit_inference_pool = exploit_inference_pool

        self.blueprint_hostports = blueprint_inference_pool.hostports
        self.exploit_hostports = exploit_inference_pool.hostports

    def _start_rollout_procs(self):
        game_json = Game().to_saved_game_format()
        rollout_cfg = self.rollout_cfg

        def _build_rollout_kwargs(proc_id):
            if not rollout_cfg.selfplay:
                mode = RolloutMode.EXPLOIT
            else:
                mode = RolloutMode.SELFPLAY
            assert rollout_cfg.blueprint_temperature > 0
            return dict(
                reward_kwargs=MessageToDict(rollout_cfg.reward, preserving_proto_field_name=True),
                rollout_kwargs=dict(
                    mode=mode,
                    initial_power_index=(proc_id % len(POWERS)),
                    blueprint_hostports=self.blueprint_hostports,
                    exploit_hostports=self.exploit_hostports,
                    temperature=rollout_cfg.blueprint_temperature,
                    game_json=game_json,
                    max_rollout_length=rollout_cfg.rollout_max_length,
                    batch_size=rollout_cfg.rollout_batch_size,
                    fast_finish=rollout_cfg.fast_finish,
                ),
                output_games=rollout_cfg.dump_games_every > 0,
            )

        if rollout_cfg.num_rollout_processes > 0:
            logging.info("Creating rollout queue")
            queue = mp.Queue(maxsize=40)
            logging.info("Creating rollout workers")
            procs = [
                ExceptionHandlingProcess(
                    target=queue_rollouts,
                    args=[queue],
                    kwargs=_build_rollout_kwargs(i),
                    daemon=True,
                )
                for i in range(rollout_cfg.num_rollout_processes)
            ]
            logging.info("Starting rollout workers")
            for p in procs:
                p.start()
            logging.info("Done")
            rollout_generator = (queue.get() for _ in itertools.count())
            # Keeping track of there to prevent garbage collection.
            self._rollout_procs = procs
        else:
            queue = None
            rollout_generator = _yield_rollouts(**_build_rollout_kwargs(0))
            self._rollout_procs = None

        self.rollout_iterator = iter(rollout_generator)
        self.queue = queue

    def _maybe_start_eval_procs(self):
        if not self.rollout_cfg.selfplay or self.rollout_cfg.num_eval_rollout_processes == 0:
            self.evaler = None
        else:
            self.evaler = Evaler(
                model_path=self.model_path,
                reward_cfg=self.rollout_cfg.reward,
                num_procs=self.rollout_cfg.num_eval_rollout_processes,
                blueprint_hostports=self.blueprint_hostports,
                exploit_hostports=self.exploit_hostports,
                temperature=self.rollout_cfg.blueprint_temperature,
            )

    def extract_eval_scores(self) -> Optional[ScoreDict]:
        if self.evaler is None:
            return None
        else:
            return self.evaler.extract_scores()

    def terminate(self):
        logging.warning(
            "DataLoader is being destroyed. Any data read before this point may misbehave"
        )
        if self._rollout_procs is not None:
            logging.info("Killing rollout processes")
            for proc in self._rollout_procs:
                proc.kill()
        if self.evaler is not None:
            self.evaler.terminate()
        self.blueprint_inference_pool.terminate()
        self.exploit_inference_pool.terminate()

    def _yield_batches(self) -> Generator[Tuple[RolloutBatch, ScoreDict], None, None]:
        rollout_cfg = self.rollout_cfg
        accumulated_batches = []
        aggregated_scores = get_default_rollout_scores()
        size = 0
        batch_size = rollout_cfg.batch_size
        for rollout_id in itertools.count():
            rollout: RolloutBatch
            scores: ScoreDict
            (rollout, scores), game_json = next(self.rollout_iterator)
            if rollout_cfg.dump_games_every and rollout_id % rollout_cfg.dump_games_every == 0:
                power_id = rollout.power_ids[0].item()
                game_dump_folder = pathlib.Path(f"dumped_games")
                game_dump_folder.mkdir(exist_ok=True, parents=True)
                dump_path = game_dump_folder / f"game.{POWERS[power_id]}.{rollout_id:06d}.json"
                with (dump_path).open("w") as stream:
                    json.dump(game_json, stream)

            size += len(rollout.rewards)
            accumulated_batches.append(rollout)
            for k, v in scores.items():
                aggregated_scores[k] += v
            if rollout_cfg.do_not_split_rollouts:
                if size >= batch_size:
                    yield _join_batches(accumulated_batches), aggregated_scores
                    # Reset.
                    accumulated_batches = []
                    aggregated_scores = get_default_rollout_scores()
                    size = 0
            elif size > batch_size:
                # Use strict > to simplify the code.
                joined_batch = _join_batches(accumulated_batches)
                while size > batch_size:
                    extracted_batch = fairdiplomacy.selfplay.metrics.rec_map(
                        lambda x: x[:batch_size], joined_batch
                    )
                    joined_batch = fairdiplomacy.selfplay.metrics.rec_map(
                        lambda x: x[batch_size - rollout_cfg.batch_interleave_size :], joined_batch
                    )
                    size -= batch_size - rollout_cfg.batch_interleave_size
                    if self.queue is not None:
                        # Hack. We want queue size to be the size of the queue when batch is produced.
                        aggregated_scores["queue_size"] = (
                            self.queue.qsize() * aggregated_scores["num_games"]
                        )
                    yield extracted_batch, aggregated_scores
                    # Reset.
                    aggregated_scores = get_default_rollout_scores()
                accumulated_batches = [joined_batch]

    def get_batch(self) -> Tuple[RolloutBatch, ScoreDict]:
        return next(self._batch_iterator)


def compute_entropy_loss(logits, mask):
    """Return the entropy loss, i.e., the negative entropy of the policy."""
    policy = torch.nn.functional.softmax(logits, dim=-1)
    log_policy = torch.nn.functional.log_softmax(logits, dim=-1)
    return torch.sum(policy * log_policy * mask.unsqueeze(-1)) / (mask.sum() + 1e-5)


def compute_policy_gradient_loss(action_logprobs, advantages):
    return -torch.mean(action_logprobs * advantages.detach())


def to_onehot(indices, max_value):
    y_onehot = torch.zeros((len(indices), max_value), device=indices.device)
    y_onehot.scatter_(1, indices.unsqueeze(1), 1)
    return y_onehot


def build_optimizer(net, optimizer_cfg):
    return torch.optim.Adam(net.parameters(), lr=optimizer_cfg.lr)


def vtrace_from_logprobs_no_batch(**kwargs):
    kwargs = {k: (v.unsqueeze(1) if v.shape else v.unsqueeze(0)) for k, v in kwargs.items()}
    ret = fairdiplomacy.selfplay.vtrace.from_importance_weights(**kwargs)
    return type(ret)(*[i.squeeze(1) for i in ret])


def clip_grad_norm_(parameters, max_norm, norm_type=2):
    """Copied from Pytorch 1.5. Faster version for grad norm."""
    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]
    parameters = list(filter(lambda p: p.grad is not None, parameters))
    max_norm = float(max_norm)
    norm_type = float(norm_type)
    total_norm = torch.norm(
        torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]), norm_type
    )
    clip_coef = max_norm / (total_norm + 1e-6)
    if clip_coef < 1:
        for p in parameters:
            p.grad.detach().mul_(clip_coef)
    return total_norm


@attr.s(auto_attribs=True)
class TrainerState:
    """
    Contains the state of the Trainer.
    It can be saved to checkpoint the training and loaded to resume it.
    """

    epoch_id: int
    global_step: int
    model: torch.nn.Module
    optimizer: torch.optim.Optimizer
    # To be able to load the model in eval.
    args: argparse.Namespace

    def state_dict(self) -> Dict:
        data = attr.asdict(self)
        data["model"] = self.model.state_dict()
        data["optimizer"] = self.optimizer.state_dict()
        return data

    def save(self, filename: pathlib.Path) -> None:
        tmp_fpath = pathlib.Path(str(filename) + ".tmp")
        torch.save(self.state_dict(), tmp_fpath)
        tmp_fpath.rename(filename)

    @classmethod
    def load(
        cls, filename: pathlib.Path, default: "TrainerState", device: str = "cpu"
    ) -> "TrainerState":
        data = torch.load(filename, map_location=device)
        if frozenset(attr.asdict(default)) != frozenset(data):
            logging.error(
                "Loading state from '%s' that has different set of keys.\n\tState keys: %s\n\tckpt keys:%s",
                filename,
                sorted(attr.asdict(default)),
                sorted(data),
            )
            raise ValueError("Bad checkpoint")
        # We need this default to load the state dict
        model = default.model
        model.load_state_dict(data["model"])
        data["model"] = model

        optimizer = default.optimizer
        optimizer.load_state_dict(data["optimizer"])
        data["optimizer"] = optimizer

        logging.info("Loaded state from %s", filename)
        logging.info(
            "Loaded scalars: %s",
            {k: v for k, v in data.items() if isinstance(v, (int, float, str))},
        )

        return cls(**data)


class ExploitTrainer:
    def __init__(self, cfg: "conf.conf_pb2.ExploitTask"):
        self.cfg = cfg
        if cfg.seed > 0:
            torch.manual_seed(cfg.seed)
        self.state: TrainerState
        self.logger: fairdiplomacy.selfplay.metrics.Logger
        self.last_epoch_state: Dict

    def _init_state(self, device):
        net = load_dipnet_model(self.cfg.model_path, map_location=device, eval=True)
        if self.cfg.reset_agent_weights:

            def _reset(module):
                if hasattr(module, "reset_parameters"):
                    module.reset_parameters()

            net.apply(_reset)

        optim = build_optimizer(net, self.cfg.optimizer)
        self.state = TrainerState(
            model=net,
            optimizer=optim,
            epoch_id=0,
            global_step=0,
            args=torch.load(self.cfg.model_path)["args"],
        )

    def on_requeue(self):
        logging.info("Pre-termination callback")
        if hasattr(self, "logger"):
            logging.info("Closing the logger")
            self.logger.close()
        if hasattr(self, "data_loader"):
            logging.info("Killing data loader")
            self.data_loader.terminate()

    def __call__(self):
        cfg = self.cfg
        device = "cuda"  # Training device.
        self._init_state(device)

        if heyhi.is_master():
            CKPT_DIR.mkdir(exist_ok=True, parents=True)
        if REQUEUE_CKPT.exists():
            logging.info("Found requeue checkpoint: %s", REQUEUE_CKPT.resolve())
            self.state = TrainerState.load(REQUEUE_CKPT, self.state, device)
        elif CKPT_DIR.exists() and list(CKPT_DIR.iterdir()):
            last_ckpt = max(CKPT_DIR.iterdir(), key=str)
            logging.info("Found existing checkpoint folder. Will load last one: %s", last_ckpt)
            self.state = TrainerState.load(last_ckpt, self.state, device)
        else:
            logging.info("No checkpoints to restart from found")

        # TODO(akhti): wipe the folder.
        # TODO(akhti): handle multi machine.
        ckpt_syncer = CkptSyncer(get_ckpt_sync_dir(), create_dir=True)
        ckpt_syncer.save_state_dict(self.state.model)

        logger = self.logger = fairdiplomacy.selfplay.metrics.Logger()
        data_loader = self.data_loader = DataLoader(cfg.model_path, cfg.rollout)
        self.state.model.train()
        if self.cfg.trainer.train_as_eval:
            self.state.model.eval()
            # Cast cuDNN RNN back to train mode.
            self.state.model.apply(_lstm_to_train)
        elif self.cfg.trainer.train_encoder_as_eval:
            self.state.model.encoder.eval()
        elif self.cfg.trainer.train_decoder_as_eval:
            self.state.model.policy_decoder.eval()
            self.state.model.policy_decoder.apply(_lstm_to_train)
        elif self.cfg.trainer.train_as_eval_but_batchnorm:
            self.state.model.eval()
            self.state.model.apply(_lstm_to_train)
            self.state.model.apply(_bn_to_train)

        max_epochs = self.cfg.trainer.max_epochs or 10 ** 9
        self.state.model.to(device)

        # Installing requeue handle at the last moment so that we don't requeue zombie job.
        heyhi.maybe_init_requeue_handler(self.on_requeue)

        for self.state.epoch_id in range(self.state.epoch_id, max_epochs):
            # Clone state each epoch in case we'll need to requeue.
            self.state.save(REQUEUE_CKPT)
            if (
                self.cfg.trainer.save_checkpoint_every
                and self.state.epoch_id % self.cfg.trainer.save_checkpoint_every == 0
            ):
                self.state.save(CKPT_DIR / (CKPT_TPL % self.state.epoch_id))
            # Coutner accumulate different statistic over the epoch. Default
            # accumulation strategy is averaging.
            counters = collections.defaultdict(fairdiplomacy.selfplay.metrics.FractionCounter)
            use_grad_clip = self.cfg.optimizer.grad_clip > 1e-10
            if use_grad_clip:
                counters[
                    "optim/grad_max"
                ] = grad_max_counter = fairdiplomacy.selfplay.metrics.MaxCounter()
            # For LR just record it's value at the start of the epoch.
            counters["optim/lr"].update(next(iter(self.state.optimizer.param_groups))["lr"])
            epoch_start_time = time.time()
            for _ in range(self.cfg.trainer.epoch_size):
                timings = TimingCtx()
                with timings("data_gen"):
                    (
                        (power_ids, obs, rewards, actions, behavior_action_logprobs, done),
                        rollout_scores,
                    ) = data_loader.get_batch()

                with timings("to_cuda"):
                    actions = actions.to(device)
                    rewards = rewards.to(device)
                    power_ids = power_ids.to(device)
                    *obs, cand_actions = [x.to(device) for x in obs]
                    behavior_action_logprobs = behavior_action_logprobs.to(device)
                    done = done.to(device)

                with timings("net"):
                    # Shape: _, [B, 17], [B, S, 469], [B, 7].
                    # policy_cand_actions has the same information as actions,
                    # but uses local indices to match policy logits.
                    assert EOS_IDX == -1, "Rewrite the code to remove the assumption"
                    _, _, policy_logits, sc_values = self.state.model(
                        *obs,
                        temperature=1.0,
                        teacher_force_orders=actions.clamp(0),  # EOS_IDX = -1 -> 0
                        x_power=to_onehot(power_ids, len(POWERS)),
                    )
                    cand_actions = cand_actions[:, : policy_logits.shape[1]]

                    # Shape: [B].
                    sc_values = sc_values.gather(1, power_ids.unsqueeze(1)).squeeze(1)

                    # Removing absolute order ids to not use them by accident.
                    # Will use relative order ids (cand_actions) from now on.
                    del actions

                    if self.cfg.rollout.do_not_split_rollouts:
                        # Asssumes that episode actually ends.
                        bootstrap_value = torch.zeros_like(sc_values[-1])
                    else:
                        # Reducing batch size by one. Deleting things that are
                        # too lazy to adjsut to avoid artifacts.
                        bootstrap_value = sc_values[-1].detach()
                        sc_values = sc_values[:-1]
                        cand_actions = cand_actions[:-1]
                        policy_logits = policy_logits[:-1]
                        rewards = rewards[:-1]
                        del obs
                        del power_ids
                        behavior_action_logprobs = behavior_action_logprobs[:-1]
                        done = done[:-1]

                    # Shape: [B].
                    discounts = (~done).float() * self.cfg.discounting

                    # Shape: [B, 17].
                    mask = (cand_actions != EOS_IDX).float()

                    policy_action_logprobs = order_logits_to_action_logprobs(
                        policy_logits, cand_actions, mask
                    )

                    vtrace_returns = vtrace_from_logprobs_no_batch(
                        log_rhos=policy_action_logprobs - behavior_action_logprobs,
                        discounts=discounts,
                        rewards=rewards,
                        values=sc_values,
                        bootstrap_value=bootstrap_value,
                    )

                    critic_mses = 0.5 * ((vtrace_returns.vs.detach() - sc_values) ** 2)

                    losses = dict(
                        actor=compute_policy_gradient_loss(
                            policy_action_logprobs, vtrace_returns.pg_advantages
                        ),
                        critic=critic_mses.mean(),
                        # TODO(akhti): it's incorrect to apply this to
                        # per-position order distribution instead of action
                        # distribution.
                        entropy=compute_entropy_loss(policy_logits, mask),
                    )

                    loss = (
                        losses["actor"]
                        + cfg.critic_weight * losses["critic"]
                        + cfg.entropy_weight * losses["entropy"]
                    )

                    self.state.optimizer.zero_grad()
                    loss.backward()

                    if use_grad_clip:
                        g_norm_tensor = clip_grad_norm_(
                            self.state.model.parameters(), self.cfg.optimizer.grad_clip
                        )

                    if (
                        not self.cfg.trainer.max_updates
                        or self.state.global_step < self.cfg.trainer.max_updates
                    ):
                        self.state.optimizer.step()
                    # Sync to make sure timing is correct.
                    loss.item()

                with timings("metrics"), torch.no_grad():
                    last_count = done.long().sum()
                    critic_end_mses = critic_mses[done].sum()

                    if use_grad_clip:
                        g_norm = g_norm_tensor.item()
                        grad_max_counter.update(g_norm)
                        counters["optim/grad_mean"].update(g_norm)
                        counters["optim/grad_clip_ratio"].update(
                            int(g_norm >= self.cfg.optimizer.grad_clip - 1e-5)
                        )
                    for key, value in losses.items():
                        counters[f"loss/{key}"].update(value)
                    counters[f"loss/total"].update(loss.item())
                    for key, value in rollout_scores.items():
                        if key != "num_games":
                            counters[f"score/{key}"].update(value, rollout_scores["num_games"])

                    counters["loss/critic_last"].update(critic_end_mses, last_count)

                    counters["reward/mean"].update(rewards.sum(), len(rewards))
                    # Rewards at the end of episodes.
                    last_sum = rewards[done].sum()
                    counters["reward/last"].update(last_sum, last_count)
                    # To match entropy loss we don't negate logprobs. So this
                    # is an estimate of the negative entropy.
                    counters["loss/entropy_sampled"].update(policy_action_logprobs.mean())

                    # Measure off-policiness.
                    counters["loss/rho"].update(
                        vtrace_returns.rhos.sum(), vtrace_returns.rhos.numel()
                    )
                    counters["loss/rhos_clipped"].update(
                        vtrace_returns.clipped_rhos.sum(), vtrace_returns.clipped_rhos.numel()
                    )

                    bsz = len(rewards)
                    counters["size/batch"].update(bsz)
                    counters["size/episode"].update(bsz, last_count)

                with timings("sync"), torch.no_grad():
                    if self.state.global_step % self.cfg.trainer.save_sync_checkpoint_every == 0:
                        ckpt_syncer.save_state_dict(self.state.model)

                # Doing outside of the context to capture the context's timing.
                for key, value in timings.items():
                    counters[f"time/{key}"].update(value)

                if (
                    self.state.global_step < 128
                    or (self.state.global_step & self.state.global_step + 1) == 0
                ):
                    logging.info(
                        "Metrics (global_step=%d): %s",
                        self.state.global_step,
                        {k: v.value() for k, v in sorted(counters.items())},
                    )
                self.state.global_step += 1

            epoch_scalars = {k: v.value() for k, v in sorted(counters.items())}
            average_batch_size = epoch_scalars["size/batch"]
            epoch_scalars["speed/loop_bps"] = self.cfg.trainer.epoch_size / (
                time.time() - epoch_start_time + 1e-5
            )
            epoch_scalars["speed/loop_eps"] = epoch_scalars["speed/loop_bps"] * average_batch_size
            # Speed for to_cuda + forward + backward.
            torch_time = epoch_scalars["time/net"] + epoch_scalars["time/to_cuda"]
            epoch_scalars["speed/train_bps"] = 1.0 / torch_time
            epoch_scalars["speed/train_eps"] = average_batch_size / torch_time

            eval_scores = data_loader.extract_eval_scores()
            if eval_scores is not None:
                for k, v in eval_scores.items():
                    epoch_scalars[f"score_eval/{k}"] = v

            logging.info("Finished epoch %d. Metrics: %s", self.state.epoch_id, epoch_scalars)
            logger.log_metrics(epoch_scalars, self.state.epoch_id)
        logging.info("End of training")
        data_loader.terminate()
        logging.info("Exiting main funcion")


def resolve_and_flatten(nested_dict_of_scalar_tensors):
    return fairdiplomacy.selfplay.metrics.flatten_dict(
        fairdiplomacy.selfplay.metrics.recursive_tensor_item(nested_dict_of_scalar_tensors)
    )


def task(cfg):
    ExploitTrainer(cfg)()


def _lstm_to_train(module):
    if module.__class__.__name__ == "LSTM":
        module.train()


def _bn_to_train(module):
    if "BatchNorm" in module.__class__.__name__:
        module.train()
